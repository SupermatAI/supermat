{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Supermat","text":""},{"location":"#preface","title":"Preface","text":"<p>Current retrieval systems face two fundamental limitations that we've accepted as normal. </p> <p>First, as they fragment information during processing, they lose natural relationships.  While vector search is powerful for finding semantically related content, similarity isn't the same as actual relationships. Even the most sophisticated similarity search can't fully reconstruct explicit relationships nor make implicit connections clear . Systems end up spending massive resources trying to approximate context through similarity and further post-processing.  The result:  increasingly sophisticated systems bogged down by trying to reconstruct what was there all along. </p> <p>Secondly, for the purpose of referencing, these systems use flat IDs - UUIDs and random strings - that can't express relationships, forcing them to maintain separate layers just to understand how information connects. Citations are an after-thought today.  </p> <p>Our aproach solves both these problems with a fundamental insight.  Information has natural connections - from documents to sections to paragraphs to sentences. This isn't arbitrary; it's how humans organize and understand knowledge. So why do we let AI systems break them apart? </p>"},{"location":"#introduction","title":"Introduction","text":"<p>Supermat inroduces a novel data representation framework for the AI era, making relationships explicit and retrievable by design.</p>"},{"location":"#structured-citations","title":"Structured Citations","text":"<p>Our Structure ID is a unique referencing system that leverages hierarchies (document, section, paragraph, and sentence) to precisely locate text. This intuitive system makes precise attribution straightforward.</p> <p>The Structure ID, goes like this <code>2.1.4.8</code>. This points specifically to Document Index number <code>2</code>, the <code>1</code>st section in that document, then the <code>4</code>th paragraph in that section, and finally the <code>8</code>th sentence in that paragraph. This simple yet powerful structure serves two purposes: it maintains connections between different text chunks while remaining token-efficient for LLM processing.</p> <p>Furthermore, through post-processing of LLM outputs, this structure enables direct verbatim retrieval of original content from source documents, reducing the need for token repetition and minimizing the risk of hallucinations.</p>"},{"location":"#evaluation","title":"Evaluation","text":"<p>We chose to work with the CUAD dataset to showcase our approach. The legal domain provides an excellent testing ground due to its complexity and ambigious language. If Supermat works here, it can be generalized to other domains. We evaluated our approach against two benchmarks:</p> <ol> <li>A baseline standard chunking strategy</li> <li>LangChain's current state-of-the-art SemanticChunker with <code>breakpoint_threshold_type=\"percentile\"</code></li> </ol> <p>Key Metrics:</p> <p>Accuracy: +15.56% | Faithfulness: +12.53% | ROUGE-1 Recall: +33.33%</p> <p>In our internal evals, we see double-digit lifts in factual correctness and broader coverage with more complete outputs. This translates to fewer hallucinations and more trust in automated answers.</p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>This is only the beginning of our journey as we build products on this foundation. </p> <p>Our findings suggest significant untapped potential in this area. Preserving and representing both explicit and implicit connections within data before any chunking or processing yields better results than immediatly jumping into chunking.</p> <p>Far beyond just efficiency gains, we beleive this crucial intersection enables simplified yet powerful human + AI interfaces and novel operating models of tomorrow. We're building for those. </p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributers and collaborators to join us on this journey. </p>"},{"location":"Evaluation/","title":"Evaluation","text":"<p>Here we demonstrate the results of our evaluation exercise.</p>"},{"location":"Evaluation/#supermat-evaluation-metrics","title":"Supermat Evaluation Metrics","text":"<p>Results of evaluation on Supermat retriever.</p> Unnamed: 0 feedback.labeled_criteria:faithful feedback.labeled_criteria:accuracy feedback.cosine_similarity feedback.rouge1_f1_score feedback.rouge1_precision feedback.rouge1_recall feedback.rouge2_f1_score feedback.rouge2_precision feedback.rouge2_recall feedback.rougeLsum_f1_score feedback.rougeLsum_precision feedback.rougeLsum_recall error execution_time count 490 491 491 491 491 491 491 491 491 491 491 491 1 492 mean 0.786327 0.853564 0.791328 0.0431129 0.0275313 0.424209 0.0142695 0.0107686 0.0355544 0.04223 0.0267061 0.423059 nan 1.4606 std 0.20909 0.221319 0.0358326 0.0937076 0.0747593 0.479402 0.0692836 0.0553136 0.164963 0.0901142 0.0702288 0.479115 nan 0.46089 min 0.2 0.1 0.717511 0 0 0 0 0 0 0 0 0 nan 0.562514 25% 0.8 0.7 0.773459 0 0 0 0 0 0 0 0 0 nan 1.14542 50% 0.9 1 0.787848 0 0 0 0 0 0 0 0 0 nan 1.39943 75% 0.9 1 0.802803 0.0533428 0.0277778 1 0 0 0 0.0533428 0.0277778 1 nan 1.71919 max 1 1 0.971989 0.666667 0.692308 1 0.625 0.588235 1 0.666667 0.634615 1 nan 3.4466"},{"location":"Evaluation/#sota-comparison","title":"SOTA Comparison","text":"<p>Aggregated Results Difference compared with Langchain SemanticChunker Percentile</p> desc feedback.labeled_criteria:faithful feedback.labeled_criteria:accuracy feedback.cosine_similarity feedback.rouge1_f1_score feedback.rouge1_precision feedback.rouge1_recall feedback.rouge2_f1_score feedback.rouge2_precision feedback.rouge2_recall feedback.rougeLsum_f1_score feedback.rougeLsum_precision feedback.rougeLsum_recall execution_time mean 0.125303 0.155652 -0.00757736 0.206762 0.126177 0.333367 0.00773695 -0.0448571 -0.0776622 0.228423 0.151071 0.343074 -0.0618311 min 0 0 0.00461873 nan nan nan nan nan nan nan nan nan -0.193716 25% 0.555556 0 -0.00499138 nan nan nan nan nan nan nan nan nan 0.0338315 50% 0.125 0.25 -0.00275206 nan nan nan nan nan nan nan nan nan -0.00420015 75% 0 0 -0.0117252 0.230769 0.236842 0 nan nan nan 0.230769 0.236842 0 -0.0840367 max 0 0 0 0 -0.0666667 0 0 -0.0271226 0 0 -0.0750751 0 -0.417665"},{"location":"Evaluation/#baseline-comparison","title":"Baseline Comparison","text":"<p>Aggregated Results Difference compared with Langchain RecursiveCharacterTextSplitter</p> desc feedback.labeled_criteria:faithful feedback.labeled_criteria:accuracy feedback.cosine_similarity feedback.rouge1_f1_score feedback.rouge1_precision feedback.rouge1_recall feedback.rouge2_f1_score feedback.rouge2_precision feedback.rouge2_recall feedback.rougeLsum_f1_score feedback.rougeLsum_precision feedback.rougeLsum_recall execution_time mean 0.118129 0.135316 -0.00548876 0.131199 0.0417132 0.307893 0.0591786 -0.00574848 0.0892132 0.171545 0.0941226 0.315888 -0.446078 min 1 0 -0.018756 nan nan nan nan nan nan nan nan nan -0.423493 25% 0.333333 0 -0.001981 nan nan nan nan nan nan nan nan nan -0.378166 50% 0.125 0 -0.00224861 nan nan nan nan nan nan nan nan nan -0.391275 75% 0 0 -0.00517427 0.200213 0.222222 0 nan nan nan 0.200213 0.222222 0 -0.42135 max 0 0 0.00623509 0 0 0 0.171875 0 0 0.166667 0 0 -0.638781"},{"location":"Installation/","title":"Setup","text":""},{"location":"Installation/#installation","title":"Installation","text":"<ol> <li>Clone this repository</li> <li>Setup python-poetry in your system.</li> <li>Run <code>poetry install --with=frontend --all-extras</code> in your virtual environment to install all required dependencies.</li> <li>In terminal run <code>python -m supermat.gradio</code> to the run the gradio interface to see it in action.</li> </ol>"},{"location":"Installation/#setting-up-adobe","title":"Setting up Adobe","text":"<p>We weren't able to capture hierarchical structure with our open source pdf parsing libraries. We are actively working on other alternatives that can parse the pdf files with the hierarchical structure.</p> <ol> <li>Setup with Adobe PDF Services API as shown here</li> <li>Provide the credentials in the <code>.env</code> file</li> <li>To cache the adobe results, you can set the <code>TMP_DIR</code> environment variable to a persistent location as well</li> </ol>"},{"location":"Retriever/","title":"Retrieval System","text":"<p>The Supermat Retriever integrates seamlessly with existing retrieval frameworks while leveraging our structured document approach. Here's how it works and what makes it unique.</p>"},{"location":"Retriever/#supermat-retriever","title":"Supermat Retriever","text":"<p>Our retriever is designed as a flexible wrapper that enhances existing vector store capabilities with Supermat's structural awareness. It currently integrates with Langchain's Vector Stores, serving as a drop-in replacement that preserves all standard functionality while adding structural context.</p>"},{"location":"Retriever/#key-features","title":"Key Features","text":"<ul> <li>Seamless Integration: Works as a direct replacement for any Langchain vector store</li> <li>Structure-Aware Retrieval: Maintains document hierarchies during the retrieval process</li> <li>Framework Flexibility: Built to support multiple retrieval frameworks</li> <li>Preserved Context: Utilizes Structure IDs to maintain document relationships</li> </ul>"},{"location":"Retriever/#implementation","title":"Implementation","text":"<p>To use the Supermat Retriever:</p> <ol> <li>Process all the documents using the <code>FileProcessor</code></li> <li>Choose your preferred vector store</li> <li>Initialize <code>SupermatRetriever</code> with your selected store and the processed documents</li> <li>Process and retrieve documents while maintaining structural integrity</li> </ol> <p>Example:</p>"},{"location":"Retriever/#step-1-process-all-your-files","title":"Step 1: Process all your files","text":"<pre><code>from itertools import chain\nfrom typing import TYPE_CHECKING, cast\n\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\n\nfrom supermat.core.parser import FileProcessor\n\nparsed_files = Parallel(n_jobs=-1, backend=\"threading\")(delayed(FileProcessor.parse_file)(path) for path in pdf_files)\n\ndocuments = list(chain.from_iterable(parsed_docs for parsed_docs in parsed_files))\n</code></pre>"},{"location":"Retriever/#step-2-take-the-processed-documents-and-build-the-retriever","title":"Step 2: Take the processed documents and build the retriever","text":"<pre><code>from langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nfrom supermat.langchain.bindings import SupermatRetriever\n\nretriever = SupermatRetriever(\n    parsed_docs=documents,\n    vector_store=Chroma(\n        embedding_function=HuggingFaceEmbeddings(\n            model_name=\"thenlper/gte-base\",\n        ),\n        persist_directory=\"./chromadb\",\n        collection_name=\"NAME\",\n    ),\n)\n</code></pre>"},{"location":"Retriever/#step-3-now-the-retriever-can-be-used-in-a-langchain-retrieval-chain","title":"Step 3. Now the retriever can be used in a Langchain retrieval chain.","text":"<p>You can use the default chain that we provide that provides citation. This prompt was built on Deepseek 8b model.</p> <pre><code>from langchain_ollama.llms import OllamaLLM\n\nfrom supermat.langchain.bindings import get_default_chain\n\nllm_model = OllamaLLM(model=\"deepseek-r1:8b\", temperature=0.0)\nchain = get_default_chain(retriever, llm_model, substitute_references=False, return_context=False)\n</code></pre>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Supermat</li> <li>Installation</li> <li>Structure</li> <li>Supermat Retriever</li> <li>Evaluation Results</li> <li>*.md</li> <li>*/</li> <li>API References</li> </ul>"},{"location":"Structure/","title":"Structure","text":""},{"location":"Structure/#high-level-overview","title":"High Level Overview","text":"<p>Supermat's architecture consists of two primary components:</p> <ol> <li> <p>FileProcessor: A sophisticated component that parses input files and transforms them into our structured <code>ParsedDocument</code> model. This model preserves the document's hierarchical structure while making it machine-processable.</p> </li> <li> <p>Chunking System: This component takes the lossless <code>ParsedDocument</code> and intelligently segments it into chunks that fit within an LLM's context length. The current version operates primarily at the paragraph level, though future iterations will explore more adaptive chunking strategies.</p> </li> </ol>"},{"location":"Structure/#design-philosophy","title":"Design Philosophy","text":"<p>Supermat's primary focus lies in sophisticated document processing rather than traditional retrieval mechanisms. Our unique Structure ID system creates hierarchical connections between different document components, enabling:</p> <ul> <li>Precise content location and retrieval</li> <li>Maintenance of context across chunks</li> <li>Efficient navigation through document hierarchies</li> </ul>"},{"location":"Structure/#current-limitations-and-future-work","title":"Current Limitations and Future Work","text":"<p>While the current implementation effectively demonstrates our approach, we acknowledge several areas for future enhancement:</p> <ul> <li>Development of more intelligent, adaptive chunking algorithms</li> <li>Optimization of chunk sizes based on content semantics</li> <li>Integration with various document formats</li> <li>Enhanced preservation of document metadata</li> </ul> <p>Our roadmap includes expanding these capabilities while maintaining the system's core strength: preserving document structure throughout the AI processing pipeline.</p>"},{"location":"Structure/FileProcessor/","title":"File Processing System","text":"<p>The <code>FileProcessor</code> module forms the foundation of Supermat's document handling capabilities, converting various document formats into our structured <code>ParsedDocument</code> model while preserving their hierarchical organization.</p>"},{"location":"Structure/FileProcessor/#architecture-components","title":"Architecture Components","text":""},{"location":"Structure/FileProcessor/#handler","title":"Handler","text":"<p>The <code>Handler</code> orchestrates document processing through two key components:</p> <ol> <li>Converters: A collection of utilities that transform various file formats into a standardized format for parsing. For example:</li> <li>Converting <code>.docx</code> to <code>.pdf</code></li> <li>Converting <code>.pptx</code> to <code>.pdf</code></li> <li> <p>Future support planned for additional formats</p> </li> <li> <p>Parser: Processes the standardized format to generate the <code>ParsedDocument</code> model.</p> </li> </ol> <p>This modular approach allows for:</p> <ul> <li>Format flexibility</li> <li>Easy integration of new document types</li> <li>Consistent parsing behavior across different input formats</li> </ul>"},{"location":"Structure/FileProcessor/#parser","title":"Parser","text":"<p>The Parser component performs the critical task of transforming documents into our structured <code>ParsedDocument</code> model while:</p> <ul> <li>Maintaining complete document fidelity (lossless conversion)</li> <li>Preserving hierarchical relationships (sections, paragraphs, sentences)</li> <li>Converting unstructured text into a structured Pydantic model</li> </ul>"},{"location":"Structure/FileProcessor/#current-implementation","title":"Current Implementation","text":"<p>We currently support PDF parsing through two powerful backends:</p> <ol> <li>PyMuPDF: An open-source PDF processing library</li> <li>Adobe PDF Services API: Professional-grade PDF processing</li> </ol>"},{"location":"Structure/FileProcessor/#document-model","title":"Document Model","text":"<p>Our <code>ParsedDocument</code> model is designed to capture the complete structure of a document while making it processable for AI pipelines. For detailed information about the model structure and capabilities, refer to our model documentation.</p>"},{"location":"Structure/FileProcessor/#future-enhancements","title":"Future Enhancements","text":"<p>We plan to expand the File Processing System with:</p> <ul> <li>Support for additional document formats</li> <li>Enhanced structure detection algorithms</li> <li>Improved metadata extraction</li> <li>Advanced formatting preservation</li> <li>Custom parsers for specialized document types</li> </ul>"},{"location":"reference/","title":"API References","text":""},{"location":"reference/core/","title":"Core","text":"<p>This is where the core supermat parsing logic exists. Core deals with Supermat's parser pydantic models to define structure to the parsed documents, chunking strategies, and parser logic to convert documents into the <code>ParsedDocument</code> model.</p>"},{"location":"reference/core/#supermat.core.export_parsed_document","title":"<code>export_parsed_document(document, output_path, **kwargs)</code>","text":"<p>Export given ParsedDocument to a json file</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>ParsedDocumentType</code> <p>The ParsedDocument to be dumped.</p> required <code>output_path</code> <code>Path | str</code> <p>JSON file location.</p> required Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>def export_parsed_document(document: ParsedDocumentType, output_path: Path | str, **kwargs):\n    \"\"\"Export given ParsedDocument to a json file\n\n    Args:\n        document (ParsedDocumentType): The ParsedDocument to be dumped.\n        output_path (Path | str): JSON file location.\n    \"\"\"\n    output_path = Path(output_path)\n    with output_path.open(\"wb+\") as fp:\n        fp.write(ParsedDocument.dump_json(document, **kwargs))\n</code></pre>"},{"location":"reference/core/#supermat.core.load_parsed_document","title":"<code>load_parsed_document(path)</code>","text":"<p>Load a json dumped <code>ParsedDocument</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>file path to the json file.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>ParsedDocument model loaded from json.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>def load_parsed_document(path: Path | str) -&gt; ParsedDocumentType:\n    \"\"\"Load a json dumped `ParsedDocument`\n\n    Args:\n        path (Path | str): file path to the json file.\n\n    Returns:\n        ParsedDocumentType: ParsedDocument model loaded from json.\n    \"\"\"\n    path = Path(path)\n    with path.open(\"rb\") as fp:\n        raw_doc: list[dict[str, Any]] | dict[str, list[dict[str, Any]]] = orjson.loads(fp.read())\n\n    if isinstance(raw_doc, dict) and len(raw_doc.keys()) == 1:\n        root_key = next(iter(raw_doc.keys()))\n        warn(f\"The json document contains a root node {next(iter(raw_doc.keys()))}.\", ValidationWarning)\n        return ParsedDocument.validate_python(raw_doc[root_key])\n    elif isinstance(raw_doc, list):\n        return ParsedDocument.validate_python(raw_doc)\n    else:\n        raise ValueError(\"Invalid JSON Format\")\n</code></pre>"},{"location":"reference/core/utils/","title":"Utils","text":""},{"location":"reference/core/utils/#supermat.core.utils.get_structure","title":"<code>get_structure(*args, min_length=3)</code>","text":"<p>Converts the structural elements into a string with padding.</p> Source code in <code>supermat/core/utils.py</code> <pre><code>def get_structure(*args: int, min_length: int = 3) -&gt; str:\n    \"\"\"Converts the structural elements into a string with padding.\"\"\"\n    if len(args) &lt; min_length:\n        args = args + (0,) * (min_length - len(args))\n    return \".\".join(map(lambda x: str(x), args))\n</code></pre>"},{"location":"reference/core/utils/#supermat.core.utils.is_subsection","title":"<code>is_subsection(subsection, section)</code>","text":"<p>Determines if one hierarchical section ID is a subsection of another.</p> <p>This function compares two hierarchical structure IDs (e.g., \"1.2.3\" and \"1.2\") to determine if the first is a subsection of the second. A section is considered a subsection if it shares all non-zero parts with its parent section and can have additional parts after the parent's sequence.</p> For example <ul> <li>\"1.2.3\" is a subsection of \"1.2\"</li> <li>\"1.2.0\" is a subsection of \"1.2\"</li> <li>\"1.3\" is not a subsection of \"1.2\"</li> <li>\"1.2\" is not a subsection of \"1.2.3\"</li> </ul> Zero parts in the section ID (e.g., \"1.2.0\") act as wildcards and will match any corresponding part in the <p>subsection.</p> <p>Parameters:</p> Name Type Description Default <code>subsection</code> <code>str</code> <p>The structure ID to check if it's a subsection              (e.g., \"1.2.3\")</p> required <code>section</code> <code>str</code> <p>The potential parent structure ID to compare against           (e.g., \"1.2\")</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if 'subsection' is a subsection of 'section', False otherwise</p> Source code in <code>supermat/core/utils.py</code> <pre><code>def is_subsection(subsection: str, section: str) -&gt; bool:\n    \"\"\"Determines if one hierarchical section ID is a subsection of another.\n\n    This function compares two hierarchical structure IDs (e.g., \"1.2.3\" and \"1.2\")\n    to determine if the first is a subsection of the second. A section is considered\n    a subsection if it shares all non-zero parts with its parent section and can have\n    additional parts after the parent's sequence.\n\n    For example:\n        - \"1.2.3\" is a subsection of \"1.2\"\n        - \"1.2.0\" is a subsection of \"1.2\"\n        - \"1.3\" is not a subsection of \"1.2\"\n        - \"1.2\" is not a subsection of \"1.2.3\"\n\n    NOTE: Zero parts in the section ID (e.g., \"1.2.0\") act as wildcards and will match any corresponding part in the\n     subsection.\n\n    Args:\n        subsection (str): The structure ID to check if it's a subsection\n                         (e.g., \"1.2.3\")\n        section (str): The potential parent structure ID to compare against\n                      (e.g., \"1.2\")\n\n    Returns:\n        bool: True if 'subsection' is a subsection of 'section', False otherwise\n    \"\"\"\n    sub_parts = split_structure(subsection)\n    sec_parts = split_structure(section)\n    if len(sec_parts) &gt; len(sub_parts):\n        return False\n    # pad with 0s\n    sec_parts = split_structure(get_structure(*sec_parts, min_length=len(sub_parts)))\n    return all(\n        sub_part &gt;= sec_part if sec_part == 0 else sub_part == sec_part\n        for sec_part, sub_part in zip(sec_parts, sub_parts)\n    )\n</code></pre>"},{"location":"reference/core/utils/#supermat.core.utils.split_structure","title":"<code>split_structure(structure)</code>","text":"<p>Retrieves the structural elements from structure id.</p> Source code in <code>supermat/core/utils.py</code> <pre><code>def split_structure(structure: str) -&gt; tuple[int, ...]:\n    \"\"\"Retrieves the structural elements from structure id.\"\"\"\n    return tuple(map(int, structure.split(\".\")))\n</code></pre>"},{"location":"reference/core/_utils/","title":"utils","text":""},{"location":"reference/core/_utils/type_finder/","title":"Type finder","text":""},{"location":"reference/core/_utils/type_finder/#supermat.core._utils.type_finder.analyze_json_file","title":"<code>analyze_json_file(file_path, target_key='type')</code>","text":"<p>Analyze a JSON file to find all type values.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the JSON file</p> required Source code in <code>supermat/core/_utils/type_finder.py</code> <pre><code>def analyze_json_file(file_path: Path, target_key: str = \"type\") -&gt; None:\n    \"\"\"\n    Analyze a JSON file to find all type values.\n\n    Args:\n        file_path: Path to the JSON file\n    \"\"\"\n    try:\n        with open(file_path, \"rb\") as file:\n            data = orjson.loads(file.read())\n\n        type_values = find_key_values(data, target_key)\n\n        print(\"\\nFound 'type' values in the JSON file:\")\n        print(\"=====================================\")\n        for parent, values in type_values.items():\n            print(f\"\\nParent context: {parent}\")\n            print(\"Type values:\")\n            for value in sorted(values):\n                print(f\"  - {value}\")\n\n        print(f\"\\nTotal unique type values found: {sum(len(values) for values in type_values.values())}\")\n\n    except FileNotFoundError:\n        print(f\"Error: File '{file_path}' not found.\")\n    except orjson.JSONDecodeError:\n        print(f\"Error: '{file_path}' is not a valid JSON file.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n</code></pre>"},{"location":"reference/core/_utils/type_finder/#supermat.core._utils.type_finder.find_key_values","title":"<code>find_key_values(data, target_key, key_values=None)</code>","text":"<p>Recursively search through nested JSON data to find all instances of a specific key and its values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The JSON data to search through</p> required <code>target_key</code> <code>str</code> <p>The key to search for</p> required <code>key_values</code> <code>dict[str, set] | None</code> <p>Dictionary to store the results (internal use for recursion)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, set]</code> <p>Dictionary with parent keys and sets of their values</p> Source code in <code>supermat/core/_utils/type_finder.py</code> <pre><code>def find_key_values(data: Any, target_key: str, key_values: dict[str, set] | None = None) -&gt; dict[str, set]:\n    \"\"\"\n    Recursively search through nested JSON data to find all instances of a specific key and its values.\n\n    Args:\n        data: The JSON data to search through\n        target_key: The key to search for\n        key_values: Dictionary to store the results (internal use for recursion)\n\n    Returns:\n        Dictionary with parent keys and sets of their values\n    \"\"\"\n    if key_values is None:\n        key_values = defaultdict(set)\n\n    if isinstance(data, dict):\n        for key, value in data.items():\n            if key == target_key:\n                # Get the parent key if possible\n                parent_key = \"root\"\n                key_values[parent_key].add(str(value))\n\n            # Recurse into nested structures\n            find_key_values(value, target_key, key_values)\n\n    elif isinstance(data, list):\n        for item in data:\n            find_key_values(item, target_key, key_values)\n\n    return key_values\n</code></pre>"},{"location":"reference/core/chunking/","title":"Chunking","text":"<p>This is where all chunking strategies on ParsedDocuments are written. Chunking strategies are strategies to best store the ParsedDocuments in a vector store or for LLM processing.</p>"},{"location":"reference/core/chunking/#supermat.core.chunking.BaseChunker","title":"<code>BaseChunker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Chunker implementations.</p> Source code in <code>supermat/core/chunking/base.py</code> <pre><code>class BaseChunker(ABC):\n    \"\"\"\n    Base class for all Chunker implementations.\n    \"\"\"\n\n    @abstractmethod\n    def create_chunks(self, processed_document: ParsedDocumentType) -&gt; DocumentChunksType:  # noqa: U100\n        \"\"\"Build chunks from the given ParsedDocument into list of ChunkDocuments.\n        This is the public class that is called for any chunking strategy.\n\n        Args:\n            processed_document (ParsedDocumentType): The processed document that needs to split into chunks.\n\n        Returns:\n            DocumentChunksType: The chunks built by the given strategy.\n        \"\"\"\n</code></pre>"},{"location":"reference/core/chunking/#supermat.core.chunking.BaseChunker.create_chunks","title":"<code>create_chunks(processed_document)</code>  <code>abstractmethod</code>","text":"<p>Build chunks from the given ParsedDocument into list of ChunkDocuments. This is the public class that is called for any chunking strategy.</p> <p>Parameters:</p> Name Type Description Default <code>processed_document</code> <code>ParsedDocumentType</code> <p>The processed document that needs to split into chunks.</p> required <p>Returns:</p> Name Type Description <code>DocumentChunksType</code> <code>DocumentChunksType</code> <p>The chunks built by the given strategy.</p> Source code in <code>supermat/core/chunking/base.py</code> <pre><code>@abstractmethod\ndef create_chunks(self, processed_document: ParsedDocumentType) -&gt; DocumentChunksType:  # noqa: U100\n    \"\"\"Build chunks from the given ParsedDocument into list of ChunkDocuments.\n    This is the public class that is called for any chunking strategy.\n\n    Args:\n        processed_document (ParsedDocumentType): The processed document that needs to split into chunks.\n\n    Returns:\n        DocumentChunksType: The chunks built by the given strategy.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/chunking/base/","title":"Base","text":""},{"location":"reference/core/chunking/base/#supermat.core.chunking.base.BaseChunker","title":"<code>BaseChunker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Chunker implementations.</p> Source code in <code>supermat/core/chunking/base.py</code> <pre><code>class BaseChunker(ABC):\n    \"\"\"\n    Base class for all Chunker implementations.\n    \"\"\"\n\n    @abstractmethod\n    def create_chunks(self, processed_document: ParsedDocumentType) -&gt; DocumentChunksType:  # noqa: U100\n        \"\"\"Build chunks from the given ParsedDocument into list of ChunkDocuments.\n        This is the public class that is called for any chunking strategy.\n\n        Args:\n            processed_document (ParsedDocumentType): The processed document that needs to split into chunks.\n\n        Returns:\n            DocumentChunksType: The chunks built by the given strategy.\n        \"\"\"\n</code></pre>"},{"location":"reference/core/chunking/base/#supermat.core.chunking.base.BaseChunker.create_chunks","title":"<code>create_chunks(processed_document)</code>  <code>abstractmethod</code>","text":"<p>Build chunks from the given ParsedDocument into list of ChunkDocuments. This is the public class that is called for any chunking strategy.</p> <p>Parameters:</p> Name Type Description Default <code>processed_document</code> <code>ParsedDocumentType</code> <p>The processed document that needs to split into chunks.</p> required <p>Returns:</p> Name Type Description <code>DocumentChunksType</code> <code>DocumentChunksType</code> <p>The chunks built by the given strategy.</p> Source code in <code>supermat/core/chunking/base.py</code> <pre><code>@abstractmethod\ndef create_chunks(self, processed_document: ParsedDocumentType) -&gt; DocumentChunksType:  # noqa: U100\n    \"\"\"Build chunks from the given ParsedDocument into list of ChunkDocuments.\n    This is the public class that is called for any chunking strategy.\n\n    Args:\n        processed_document (ParsedDocumentType): The processed document that needs to split into chunks.\n\n    Returns:\n        DocumentChunksType: The chunks built by the given strategy.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/chunking/simple_chunking/","title":"Simple chunking","text":""},{"location":"reference/core/chunking/simple_chunking/#supermat.core.chunking.simple_chunking.SimpleChunker","title":"<code>SimpleChunker</code>","text":"<p>               Bases: <code>BaseChunker</code></p> <p>A simple chunking strategy that simply takes all TextChunks in the parsed document and converts them into chunks.</p> Source code in <code>supermat/core/chunking/simple_chunking.py</code> <pre><code>class SimpleChunker(BaseChunker):\n    \"\"\"\n    A simple chunking strategy that simply takes all TextChunks in the parsed document and converts them into chunks.\n    \"\"\"\n\n    @staticmethod\n    def build_chunk(doc_id: int, section: ChunkModelType) -&gt; ChunkDocument:\n        assert isinstance(section, BaseTextChunk)\n        assert section.properties\n        return ChunkDocument(\n            document_id=doc_id,\n            text=section.text,\n            metadata=BaseChunkMetadata(\n                type=section.type_,\n                structure=section.structure,\n                page_number=section.properties.page,\n                source=section.properties.path,\n                chunk_meta=section,\n            ),\n        )\n\n    def create_chunks(self, processed_document: ParsedDocumentType) -&gt; DocumentChunksType:\n        return [\n            SimpleChunker.build_chunk(doc_id, section)\n            for doc_id, section in enumerate(processed_document)\n            if isinstance(section, BaseTextChunk)\n        ]\n</code></pre>"},{"location":"reference/core/models/","title":"Models","text":"<p>The pydantic models which will be used to converted the unstructured format of documents into a uniform strucutre.</p>"},{"location":"reference/core/models/base_chunk/","title":"Base chunk","text":"<p>Pydantic model for parsed document chunking stategies.</p>"},{"location":"reference/core/models/parsed_document/","title":"Parsed document","text":""},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.BaseChunk","title":"<code>BaseChunk</code>","text":"<p>               Bases: <code>CustomBaseModel</code></p> <p>BaseChunk that contains the type and structure of the chunk.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class BaseChunk(CustomBaseModel):\n    \"\"\"BaseChunk that contains the type and structure of the chunk.\"\"\"\n\n    type_: Literal[\"Text\", \"Image\", \"Footnote\"] = Field(alias=\"type\", frozen=True)\n    structure: str\n    document: str | None = None\n\n    @overload\n    def has_subsection(self, sub_section: BaseChunk) -&gt; bool: ...  # noqa: U100, E704\n\n    @overload\n    def has_subsection(self, sub_section: str) -&gt; bool: ...  # noqa: U100, E704\n\n    def has_subsection(self, sub_section: BaseChunk | str) -&gt; bool:\n        return is_subsection(sub_section if isinstance(sub_section, str) else sub_section.structure, self.structure)\n\n    @overload\n    def is_subsection(self, section: BaseChunk) -&gt; bool: ...  # noqa: U100, E704\n\n    @overload\n    def is_subsection(self, section: str) -&gt; bool: ...  # noqa: U100, E704\n\n    def is_subsection(self, section: BaseChunk | str) -&gt; bool:\n        return is_subsection(self.structure, section if isinstance(section, str) else section.structure)\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.BaseChunkProperty","title":"<code>BaseChunkProperty</code>","text":"<p>               Bases: <code>CustomBaseModel</code></p> <p>Properties assosciated with a chunk. Close to adobe's format.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class BaseChunkProperty(CustomBaseModel):\n    \"\"\"Properties assosciated with a chunk. Close to adobe's format.\"\"\"\n\n    object_id: int | None = Field(default=None, validation_alias=AliasChoices(\"ObjectID\", \"ObjectId\"))\n    bounds: tuple[float | int, float | int, float | int, float | int] = Field(validation_alias=\"Bounds\")\n    page: int = Field(validation_alias=\"Page\")\n    path: str | None = Field(default=None, validation_alias=\"Path\")\n    attributes: dict[str, Any] | None = None\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.BaseTextChunk","title":"<code>BaseTextChunk</code>","text":"<p>               Bases: <code>BaseChunk</code></p> <p>Common TextChunk model.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class BaseTextChunk(BaseChunk):\n    \"\"\"Common TextChunk model.\"\"\"\n\n    text: str\n    key: list[str]\n    properties: BaseChunkProperty | None = None\n    sentences: Sequence[ChunkModelForwardRefType] | None = None\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.CustomBaseModel","title":"<code>CustomBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>BaseModel with some extra tweaks. Needed this to handle previous output of parsed documents which has optional keys and needed to be saved for tests.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class CustomBaseModel(BaseModel):\n    \"\"\"\n    BaseModel with some extra tweaks.\n    Needed this to handle previous output of parsed documents which has optional keys and needed to be saved for tests.\n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True, json_schema_extra={\"by_alias\": True}, extra=\"forbid\")\n    _original_alias: dict[str, str] = PrivateAttr()\n    _unexisted_keys: set[str] = PrivateAttr()\n\n    def __init__(self, **data: dict[str, Any]):\n        aliases: dict[str, str] = {}\n        unexisted_keys: set[str] = set()\n        for field_name, field in self.model_fields.items():\n            alias_found = False\n            if isinstance(field.validation_alias, AliasChoices):\n                for alias in field.validation_alias.choices:\n                    if alias in data:\n                        aliases[field_name] = alias\n                        alias_found = True\n                        break\n            elif field.alias is not None or field.validation_alias is not None:\n                alias = field.alias or field.validation_alias\n                if TYPE_CHECKING:\n                    assert isinstance(alias, str)\n                aliases[field_name] = alias\n                alias_found = True\n\n            if not ((alias_found and aliases[field_name] in data) or (field_name in data)):\n                unexisted_keys.add(aliases[field_name] if alias_found else field_name)\n\n        super().__init__(**data)\n        self._original_alias = aliases\n        self._unexisted_keys = unexisted_keys\n\n    @model_serializer(mode=\"wrap\")\n    def serialize_model(self, nxt: SerializerFunctionWrapHandler) -&gt; dict[str, Any]:\n        \"\"\"This custom serializer ensures that extra keys are included as well.\"\"\"\n        serialized = nxt(self)\n        aliased_values = {\n            renamed_field_name: serialized.pop(field_name)\n            for field_name, renamed_field_name in self._original_alias.items()\n            if field_name in serialized\n        }\n        serialized.update(aliased_values)\n        _unexisted_keys = self._unexisted_keys - {\n            field.alias or field_name for field_name, field in self.model_fields.items() if field.frozen\n        }\n        cleaned_serialized = {\n            field_name: value for field_name, value in serialized.items() if field_name not in _unexisted_keys\n        }\n        return cleaned_serialized\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.CustomBaseModel.serialize_model","title":"<code>serialize_model(nxt)</code>","text":"<p>This custom serializer ensures that extra keys are included as well.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>@model_serializer(mode=\"wrap\")\ndef serialize_model(self, nxt: SerializerFunctionWrapHandler) -&gt; dict[str, Any]:\n    \"\"\"This custom serializer ensures that extra keys are included as well.\"\"\"\n    serialized = nxt(self)\n    aliased_values = {\n        renamed_field_name: serialized.pop(field_name)\n        for field_name, renamed_field_name in self._original_alias.items()\n        if field_name in serialized\n    }\n    serialized.update(aliased_values)\n    _unexisted_keys = self._unexisted_keys - {\n        field.alias or field_name for field_name, field in self.model_fields.items() if field.frozen\n    }\n    cleaned_serialized = {\n        field_name: value for field_name, value in serialized.items() if field_name not in _unexisted_keys\n    }\n    return cleaned_serialized\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.FontProperties","title":"<code>FontProperties</code>","text":"<p>               Bases: <code>CustomBaseModel</code></p> <p>Font properties in a TextChunkProperty.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class FontProperties(CustomBaseModel):\n    \"\"\"Font properties in a TextChunkProperty.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n    alt_family_name: str | None = None\n    embedded: bool | None = None\n    encoding: str | None = None\n    family_name: str | None = None\n    font_type: str | None = None\n    italic: bool | None = None\n    monospaced: bool | None = None\n    name: str\n    subset: bool | None = None\n    weight: int | None = None\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.FootnoteChunk","title":"<code>FootnoteChunk</code>","text":"<p>               Bases: <code>TextChunk</code></p> <p>TextChunk which is a Footnote</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class FootnoteChunk(TextChunk):\n    \"\"\"TextChunk which is a Footnote\"\"\"\n\n    type_: Literal[\"Footnote\"] = Field(  # pyright: ignore[reportIncompatibleVariableOverride]\n        default=\"Footnote\", alias=\"type\", frozen=True\n    )\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.ImageChunk","title":"<code>ImageChunk</code>","text":"<p>               Bases: <code>BaseChunk</code>, <code>BaseChunkProperty</code></p> <p>ImageChunk that stores the image in Base64 encoding.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class ImageChunk(BaseChunk, BaseChunkProperty):\n    \"\"\"ImageChunk that stores the image in Base64 encoding.\"\"\"\n\n    type_: Literal[\"Image\"] = Field(  # pyright: ignore[reportIncompatibleVariableOverride]\n        default=\"Image\", alias=\"type\", frozen=True\n    )\n    figure: str | None = None\n    figure_object: Base64Bytes | None = Field(validation_alias=\"figure-object\", repr=False)\n\n    @field_validator(\"figure_object\", mode=\"before\")\n    @classmethod\n    def validate_data(cls, value: Base64Bytes | None, info: ValidationInfo):  # noqa: U100\n        # TODO (@legendof-selda): figure out a way to find the path where this fails.\n        # NOTE: This shouldn't be allowed, but in the sample we have a case where the images aren't saved.\n        if value is None:\n            warn(f\"{info.field_name} is None.\", ValidationWarning)\n            return None\n        return value\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.TextChunk","title":"<code>TextChunk</code>","text":"<p>               Bases: <code>BaseTextChunk</code></p> <p>TextChunk which was similar to the initial version of supermat.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class TextChunk(BaseTextChunk):\n    \"\"\"TextChunk which was similar to the initial version of supermat.\"\"\"\n\n    type_: Literal[\"Text\"] = Field(  # pyright: ignore[reportIncompatibleVariableOverride]\n        default=\"Text\", alias=\"type\", frozen=True\n    )\n    speaker: dict[str, Any] | None = None\n    timestamp: str | None = None\n    annotations: list[str] | None = None\n    properties: TextChunkProperty | None = None  # pyright: ignore[reportIncompatibleVariableOverride]\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.TextChunkProperty","title":"<code>TextChunkProperty</code>","text":"<p>               Bases: <code>BaseChunkProperty</code></p> <p>Properties assosciated to a TextChunk</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class TextChunkProperty(BaseChunkProperty):\n    \"\"\"Properties assosciated to a TextChunk\"\"\"\n\n    font: FontProperties = Field(validation_alias=\"Font\")\n    hasclip: bool | None = Field(default=None, validation_alias=\"HasClip\")\n    lang: str | None = Field(default=None, validation_alias=\"Lang\")\n    text_size: float | int = Field(validation_alias=\"TextSize\")\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.ValidationWarning","title":"<code>ValidationWarning</code>","text":"<p>               Bases: <code>UserWarning</code></p> <p>Custom warning for validation issues in Pydantic models.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>class ValidationWarning(UserWarning):\n    \"\"\"Custom warning for validation issues in Pydantic models.\"\"\"\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.export_parsed_document","title":"<code>export_parsed_document(document, output_path, **kwargs)</code>","text":"<p>Export given ParsedDocument to a json file</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>ParsedDocumentType</code> <p>The ParsedDocument to be dumped.</p> required <code>output_path</code> <code>Path | str</code> <p>JSON file location.</p> required Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>def export_parsed_document(document: ParsedDocumentType, output_path: Path | str, **kwargs):\n    \"\"\"Export given ParsedDocument to a json file\n\n    Args:\n        document (ParsedDocumentType): The ParsedDocument to be dumped.\n        output_path (Path | str): JSON file location.\n    \"\"\"\n    output_path = Path(output_path)\n    with output_path.open(\"wb+\") as fp:\n        fp.write(ParsedDocument.dump_json(document, **kwargs))\n</code></pre>"},{"location":"reference/core/models/parsed_document/#supermat.core.models.parsed_document.load_parsed_document","title":"<code>load_parsed_document(path)</code>","text":"<p>Load a json dumped <code>ParsedDocument</code></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>file path to the json file.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>ParsedDocument model loaded from json.</p> Source code in <code>supermat/core/models/parsed_document.py</code> <pre><code>def load_parsed_document(path: Path | str) -&gt; ParsedDocumentType:\n    \"\"\"Load a json dumped `ParsedDocument`\n\n    Args:\n        path (Path | str): file path to the json file.\n\n    Returns:\n        ParsedDocumentType: ParsedDocument model loaded from json.\n    \"\"\"\n    path = Path(path)\n    with path.open(\"rb\") as fp:\n        raw_doc: list[dict[str, Any]] | dict[str, list[dict[str, Any]]] = orjson.loads(fp.read())\n\n    if isinstance(raw_doc, dict) and len(raw_doc.keys()) == 1:\n        root_key = next(iter(raw_doc.keys()))\n        warn(f\"The json document contains a root node {next(iter(raw_doc.keys()))}.\", ValidationWarning)\n        return ParsedDocument.validate_python(raw_doc[root_key])\n    elif isinstance(raw_doc, list):\n        return ParsedDocument.validate_python(raw_doc)\n    else:\n        raise ValueError(\"Invalid JSON Format\")\n</code></pre>"},{"location":"reference/core/parser/","title":"Parser","text":"<p>The parser submodule contains all Parser implementation that converts a given file type to a ParsedDocument. For the Parser to be registered, it needs to be included here. TODO (@legendof-selda): Dynamically register all parsers.</p> <p>To create a new <code>Parser</code>, create a submodule for it and inside the submodule, it should have <code>parser.py</code>. Here is where the <code>Parser</code> implementation will be written. For any utilities associated to that parser will go to <code>utils.py</code>. Also include import the Parser in its corresponding <code>__init__.py</code> file for easier importing.</p>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor","title":"<code>FileProcessor</code>","text":"Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>class FileProcessor:\n    _registered_handlers: dict[str, Handler] = {}\n    _handlers: dict[str, list[str]] = defaultdict(list)\n    _main_handlers: dict[str, str] = {}\n    _file_extension_pattern = re.compile(r\"^\\.[a-zA-Z0-9]+(?:\\.[a-zA-Z0-9]+)*$\")\n\n    @staticmethod\n    def _register(handler: Handler, extension: str, main: bool):\n        FileProcessor._registered_handlers[handler.name] = handler\n        FileProcessor._handlers[extension].append(handler.name)\n        if main:\n            FileProcessor._main_handlers[extension] = handler.name\n\n    @staticmethod\n    def register(\n        extension: str, *, converters: type[Converter] | Iterable[type[Converter]] | None = None, main: bool = False\n    ) -&gt; Callable[[P], P]:\n        \"\"\"A `register` decorator that registers a `Parser` to specified document `extension` type\n        and the list of `Converter`s that needs to run beforing parsing the document.\n\n        Example:\n\n        ```python\n        @FileProcessor.register(\".html\")\n        @FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n        @FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\n        class HTMLParser(Parser):\n            def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n                ...\n        ```\n\n        Args:\n            extension (str): The file extension that the parser will handle.\n            converters (type[Converter] | Iterable[type[Converter]] | None, optional):\n                List of `Converter`s that converts a given file first before parsing it. Defaults to None.\n            main (bool, optional): Specifies if the decorated `Parser` is the 'main' parser for this extension type.\n                You can have multiple parsers for the same extension but only one of them can be the 'main' one.\n                Defaults to False.\n\n        Returns:\n            Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser\n        \"\"\"\n        # NOTE: this only works if the register has reached. Meaning we need to manually import it in __init__.py\n        extension = extension.lower()\n        if not extension.startswith(\".\"):\n            extension = f\".{extension}\"\n        if not FileProcessor._file_extension_pattern.match(extension):\n            raise ValueError(f\"Invalid file extension: {extension}\")\n        if converters is not None and not isinstance(converters, Iterable):\n            converters = (converters,)\n        if converters is not None and (\n            not_converters := [converter for converter in converters if not issubclass(converter, Converter)]\n        ):\n            raise TypeError(f\"{not_converters} are not subclasses of {Converter}\")\n        if main and extension in FileProcessor._main_handlers:\n            raise ValueError(\n                f\"{extension} is already registered to {FileProcessor._main_handlers[extension]}! \"\n                \"Only one main parser can be registered for given extension.\"\n            )\n\n        def decorator(parser: P) -&gt; P:\n            if not issubclass(parser, Parser):\n                raise TypeError(f\"{parser} is not a subclass of {Parser}\")\n            handler = Handler(\n                parser=parser(), converters=tuple(converter() for converter in converters) if converters else None\n            )\n            FileProcessor._register(handler, extension, main=main)\n            return parser\n\n        return decorator\n\n    @staticmethod\n    def get_main_handler(file_path: Path | str) -&gt; Handler:\n        \"\"\"Get the 'main' handler that can handle the given file.\n\n        Args:\n            file_path (Path | str): The file that needs to be handled.\n\n        Returns:\n            Handler: The main handler associated with this file type.\n        \"\"\"\n        file_path = Path(file_path)\n        file_ext = file_path.suffix.lower()\n\n        handler_id = FileProcessor._main_handlers.get(file_ext, None)\n        if handler_id is None:\n            raise ValueError(f\"No main handler registered for file type: {file_ext}\")\n\n        return FileProcessor._registered_handlers[handler_id]\n\n    @staticmethod\n    def get_handler(handler_name: str) -&gt; Handler:\n        \"\"\"Retrieve the registered handler from the given `handler_name`.\n\n        Args:\n            handler_name (str): Unique name given to the registered `Handler`.\n\n        Returns:\n            Handler: The registered `Handler`.\n        \"\"\"\n        return FileProcessor._registered_handlers[handler_name]\n\n    @staticmethod\n    def get_handlers(file_path: Path | str) -&gt; dict[str, Handler]:\n        \"\"\"Get all the handlers that can handle the given file.\n\n        Args:\n            file_path (Path | str): The file that needs to be handled.\n\n        Returns:\n            dict[str, Handler]: The handlers associated with this file type.\n        \"\"\"\n        file_path = Path(file_path)\n        file_ext = file_path.suffix.lower()\n\n        return {\n            handle_name: FileProcessor.get_handler(handle_name)\n            for handle_name in FileProcessor._handlers.get(file_ext, [])\n        }\n\n    @staticmethod\n    def parse_file(file_path: Path | str) -&gt; ParsedDocumentType:\n        \"\"\"Parses a file and returns the `ParsedDocument` after retrieving the 'main' handler for it.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            ParsedDocumentType: The parsed format of the file.\n        \"\"\"\n        handler = FileProcessor.get_main_handler(file_path)\n        return handler.parse_file(file_path)\n\n    @staticmethod\n    def process_file(file_path: Path | str, **kwargs) -&gt; Path:\n        \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it\n        after retrieving the 'main' handler for it.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n        \"\"\"\n        handler = FileProcessor.get_main_handler(file_path)\n        return handler.process_file(file_path, **kwargs)\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.get_handler","title":"<code>get_handler(handler_name)</code>  <code>staticmethod</code>","text":"<p>Retrieve the registered handler from the given <code>handler_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>handler_name</code> <code>str</code> <p>Unique name given to the registered <code>Handler</code>.</p> required <p>Returns:</p> Name Type Description <code>Handler</code> <code>Handler</code> <p>The registered <code>Handler</code>.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_handler(handler_name: str) -&gt; Handler:\n    \"\"\"Retrieve the registered handler from the given `handler_name`.\n\n    Args:\n        handler_name (str): Unique name given to the registered `Handler`.\n\n    Returns:\n        Handler: The registered `Handler`.\n    \"\"\"\n    return FileProcessor._registered_handlers[handler_name]\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.get_handlers","title":"<code>get_handlers(file_path)</code>  <code>staticmethod</code>","text":"<p>Get all the handlers that can handle the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file that needs to be handled.</p> required <p>Returns:</p> Type Description <code>dict[str, Handler]</code> <p>dict[str, Handler]: The handlers associated with this file type.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_handlers(file_path: Path | str) -&gt; dict[str, Handler]:\n    \"\"\"Get all the handlers that can handle the given file.\n\n    Args:\n        file_path (Path | str): The file that needs to be handled.\n\n    Returns:\n        dict[str, Handler]: The handlers associated with this file type.\n    \"\"\"\n    file_path = Path(file_path)\n    file_ext = file_path.suffix.lower()\n\n    return {\n        handle_name: FileProcessor.get_handler(handle_name)\n        for handle_name in FileProcessor._handlers.get(file_ext, [])\n    }\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.get_main_handler","title":"<code>get_main_handler(file_path)</code>  <code>staticmethod</code>","text":"<p>Get the 'main' handler that can handle the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file that needs to be handled.</p> required <p>Returns:</p> Name Type Description <code>Handler</code> <code>Handler</code> <p>The main handler associated with this file type.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_main_handler(file_path: Path | str) -&gt; Handler:\n    \"\"\"Get the 'main' handler that can handle the given file.\n\n    Args:\n        file_path (Path | str): The file that needs to be handled.\n\n    Returns:\n        Handler: The main handler associated with this file type.\n    \"\"\"\n    file_path = Path(file_path)\n    file_ext = file_path.suffix.lower()\n\n    handler_id = FileProcessor._main_handlers.get(file_ext, None)\n    if handler_id is None:\n        raise ValueError(f\"No main handler registered for file type: {file_ext}\")\n\n    return FileProcessor._registered_handlers[handler_id]\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.parse_file","title":"<code>parse_file(file_path)</code>  <code>staticmethod</code>","text":"<p>Parses a file and returns the <code>ParsedDocument</code> after retrieving the 'main' handler for it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>The parsed format of the file.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef parse_file(file_path: Path | str) -&gt; ParsedDocumentType:\n    \"\"\"Parses a file and returns the `ParsedDocument` after retrieving the 'main' handler for it.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        ParsedDocumentType: The parsed format of the file.\n    \"\"\"\n    handler = FileProcessor.get_main_handler(file_path)\n    return handler.parse_file(file_path)\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.process_file","title":"<code>process_file(file_path, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Parses a file and saves the <code>ParsedDocument</code> json and returns the file path to it after retrieving the 'main' handler for it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the json exported <code>ParsedDocument</code> which is nearby the given <code>file_path</code>.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef process_file(file_path: Path | str, **kwargs) -&gt; Path:\n    \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it\n    after retrieving the 'main' handler for it.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n    \"\"\"\n    handler = FileProcessor.get_main_handler(file_path)\n    return handler.process_file(file_path, **kwargs)\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.FileProcessor.register","title":"<code>register(extension, *, converters=None, main=False)</code>  <code>staticmethod</code>","text":"<p>A <code>register</code> decorator that registers a <code>Parser</code> to specified document <code>extension</code> type and the list of <code>Converter</code>s that needs to run beforing parsing the document.</p> <p>Example:</p> <pre><code>@FileProcessor.register(\".html\")\n@FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n@FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\nclass HTMLParser(Parser):\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension that the parser will handle.</p> required <code>converters</code> <code>type[Converter] | Iterable[type[Converter]] | None</code> <p>List of <code>Converter</code>s that converts a given file first before parsing it. Defaults to None.</p> <code>None</code> <code>main</code> <code>bool</code> <p>Specifies if the decorated <code>Parser</code> is the 'main' parser for this extension type. You can have multiple parsers for the same extension but only one of them can be the 'main' one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[P], P]</code> <p>Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef register(\n    extension: str, *, converters: type[Converter] | Iterable[type[Converter]] | None = None, main: bool = False\n) -&gt; Callable[[P], P]:\n    \"\"\"A `register` decorator that registers a `Parser` to specified document `extension` type\n    and the list of `Converter`s that needs to run beforing parsing the document.\n\n    Example:\n\n    ```python\n    @FileProcessor.register(\".html\")\n    @FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n    @FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\n    class HTMLParser(Parser):\n        def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n            ...\n    ```\n\n    Args:\n        extension (str): The file extension that the parser will handle.\n        converters (type[Converter] | Iterable[type[Converter]] | None, optional):\n            List of `Converter`s that converts a given file first before parsing it. Defaults to None.\n        main (bool, optional): Specifies if the decorated `Parser` is the 'main' parser for this extension type.\n            You can have multiple parsers for the same extension but only one of them can be the 'main' one.\n            Defaults to False.\n\n    Returns:\n        Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser\n    \"\"\"\n    # NOTE: this only works if the register has reached. Meaning we need to manually import it in __init__.py\n    extension = extension.lower()\n    if not extension.startswith(\".\"):\n        extension = f\".{extension}\"\n    if not FileProcessor._file_extension_pattern.match(extension):\n        raise ValueError(f\"Invalid file extension: {extension}\")\n    if converters is not None and not isinstance(converters, Iterable):\n        converters = (converters,)\n    if converters is not None and (\n        not_converters := [converter for converter in converters if not issubclass(converter, Converter)]\n    ):\n        raise TypeError(f\"{not_converters} are not subclasses of {Converter}\")\n    if main and extension in FileProcessor._main_handlers:\n        raise ValueError(\n            f\"{extension} is already registered to {FileProcessor._main_handlers[extension]}! \"\n            \"Only one main parser can be registered for given extension.\"\n        )\n\n    def decorator(parser: P) -&gt; P:\n        if not issubclass(parser, Parser):\n            raise TypeError(f\"{parser} is not a subclass of {Parser}\")\n        handler = Handler(\n            parser=parser(), converters=tuple(converter() for converter in converters) if converters else None\n        )\n        FileProcessor._register(handler, extension, main=main)\n        return parser\n\n    return decorator\n</code></pre>"},{"location":"reference/core/parser/#supermat.core.parser.PyMuPDFParser","title":"<code>PyMuPDFParser</code>","text":"<p>               Bases: <code>Parser</code></p> <p>Parses a pdf file using PyMuPDF library.</p> Source code in <code>supermat/core/parser/pymupdf_parser/parser.py</code> <pre><code>@FileProcessor.register(\".pdf\")\nclass PyMuPDFParser(Parser):\n    \"\"\"Parses a pdf file using PyMuPDF library.\"\"\"\n\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        parsed_pdf = parse_pdf(file_path)\n        return process_pymupdf(parsed_pdf)\n</code></pre>"},{"location":"reference/core/parser/base/","title":"Base","text":"<p>Base abstractions of Parser and Converters. <code>Parser</code> parses a given document type into a <code>ParsedDocumentType</code>. <code>Converter</code> converts a given document from one format to another so that it can be compatible with an existing <code>Parser</code>. Example: We have a <code>Parser</code> that parses a .pdf document, we can have <code>Converter</code>s that convert docx, pptx into pdf.</p>"},{"location":"reference/core/parser/base/#supermat.core.parser.base.Converter","title":"<code>Converter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>supermat/core/parser/base.py</code> <pre><code>class Converter(ABC):\n    @abstractmethod\n    def convert(self, file_path: Path) -&gt; Path:  # noqa: U100\n        \"\"\"\n        Converts input file to another file type and saves it. The saved file path is returned.\n\n        Args:\n            file_path (Path): Input file.\n\n        Returns:\n            Path: Output file after conversion.\n        \"\"\"\n\n    def __call__(self, file_path: Path) -&gt; Path:\n        return self.convert(file_path)\n</code></pre>"},{"location":"reference/core/parser/base/#supermat.core.parser.base.Converter.convert","title":"<code>convert(file_path)</code>  <code>abstractmethod</code>","text":"<p>Converts input file to another file type and saves it. The saved file path is returned.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Input file.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Output file after conversion.</p> Source code in <code>supermat/core/parser/base.py</code> <pre><code>@abstractmethod\ndef convert(self, file_path: Path) -&gt; Path:  # noqa: U100\n    \"\"\"\n    Converts input file to another file type and saves it. The saved file path is returned.\n\n    Args:\n        file_path (Path): Input file.\n\n    Returns:\n        Path: Output file after conversion.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/parser/base/#supermat.core.parser.base.Parser","title":"<code>Parser</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>supermat/core/parser/base.py</code> <pre><code>class Parser(ABC):\n    @abstractmethod\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:  # noqa: U100\n        \"\"\"\n        Parse give file to ParsedDocumentType.\n\n        Args:\n            file_path (Path): Input file.\n\n        Returns:\n            ParsedDocumentType: Parsed document\n        \"\"\"\n</code></pre>"},{"location":"reference/core/parser/base/#supermat.core.parser.base.Parser.parse","title":"<code>parse(file_path)</code>  <code>abstractmethod</code>","text":"<p>Parse give file to ParsedDocumentType.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Input file.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>Parsed document</p> Source code in <code>supermat/core/parser/base.py</code> <pre><code>@abstractmethod\ndef parse(self, file_path: Path) -&gt; ParsedDocumentType:  # noqa: U100\n    \"\"\"\n    Parse give file to ParsedDocumentType.\n\n    Args:\n        file_path (Path): Input file.\n\n    Returns:\n        ParsedDocumentType: Parsed document\n    \"\"\"\n</code></pre>"},{"location":"reference/core/parser/file_processor/","title":"File processor","text":"<p>FileProcessor provides an easy to use API to convert any given document by selecting it's appropriate <code>Parser</code> after it is converted to a compatible format using the <code>Converter</code>.</p>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor","title":"<code>FileProcessor</code>","text":"Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>class FileProcessor:\n    _registered_handlers: dict[str, Handler] = {}\n    _handlers: dict[str, list[str]] = defaultdict(list)\n    _main_handlers: dict[str, str] = {}\n    _file_extension_pattern = re.compile(r\"^\\.[a-zA-Z0-9]+(?:\\.[a-zA-Z0-9]+)*$\")\n\n    @staticmethod\n    def _register(handler: Handler, extension: str, main: bool):\n        FileProcessor._registered_handlers[handler.name] = handler\n        FileProcessor._handlers[extension].append(handler.name)\n        if main:\n            FileProcessor._main_handlers[extension] = handler.name\n\n    @staticmethod\n    def register(\n        extension: str, *, converters: type[Converter] | Iterable[type[Converter]] | None = None, main: bool = False\n    ) -&gt; Callable[[P], P]:\n        \"\"\"A `register` decorator that registers a `Parser` to specified document `extension` type\n        and the list of `Converter`s that needs to run beforing parsing the document.\n\n        Example:\n\n        ```python\n        @FileProcessor.register(\".html\")\n        @FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n        @FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\n        class HTMLParser(Parser):\n            def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n                ...\n        ```\n\n        Args:\n            extension (str): The file extension that the parser will handle.\n            converters (type[Converter] | Iterable[type[Converter]] | None, optional):\n                List of `Converter`s that converts a given file first before parsing it. Defaults to None.\n            main (bool, optional): Specifies if the decorated `Parser` is the 'main' parser for this extension type.\n                You can have multiple parsers for the same extension but only one of them can be the 'main' one.\n                Defaults to False.\n\n        Returns:\n            Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser\n        \"\"\"\n        # NOTE: this only works if the register has reached. Meaning we need to manually import it in __init__.py\n        extension = extension.lower()\n        if not extension.startswith(\".\"):\n            extension = f\".{extension}\"\n        if not FileProcessor._file_extension_pattern.match(extension):\n            raise ValueError(f\"Invalid file extension: {extension}\")\n        if converters is not None and not isinstance(converters, Iterable):\n            converters = (converters,)\n        if converters is not None and (\n            not_converters := [converter for converter in converters if not issubclass(converter, Converter)]\n        ):\n            raise TypeError(f\"{not_converters} are not subclasses of {Converter}\")\n        if main and extension in FileProcessor._main_handlers:\n            raise ValueError(\n                f\"{extension} is already registered to {FileProcessor._main_handlers[extension]}! \"\n                \"Only one main parser can be registered for given extension.\"\n            )\n\n        def decorator(parser: P) -&gt; P:\n            if not issubclass(parser, Parser):\n                raise TypeError(f\"{parser} is not a subclass of {Parser}\")\n            handler = Handler(\n                parser=parser(), converters=tuple(converter() for converter in converters) if converters else None\n            )\n            FileProcessor._register(handler, extension, main=main)\n            return parser\n\n        return decorator\n\n    @staticmethod\n    def get_main_handler(file_path: Path | str) -&gt; Handler:\n        \"\"\"Get the 'main' handler that can handle the given file.\n\n        Args:\n            file_path (Path | str): The file that needs to be handled.\n\n        Returns:\n            Handler: The main handler associated with this file type.\n        \"\"\"\n        file_path = Path(file_path)\n        file_ext = file_path.suffix.lower()\n\n        handler_id = FileProcessor._main_handlers.get(file_ext, None)\n        if handler_id is None:\n            raise ValueError(f\"No main handler registered for file type: {file_ext}\")\n\n        return FileProcessor._registered_handlers[handler_id]\n\n    @staticmethod\n    def get_handler(handler_name: str) -&gt; Handler:\n        \"\"\"Retrieve the registered handler from the given `handler_name`.\n\n        Args:\n            handler_name (str): Unique name given to the registered `Handler`.\n\n        Returns:\n            Handler: The registered `Handler`.\n        \"\"\"\n        return FileProcessor._registered_handlers[handler_name]\n\n    @staticmethod\n    def get_handlers(file_path: Path | str) -&gt; dict[str, Handler]:\n        \"\"\"Get all the handlers that can handle the given file.\n\n        Args:\n            file_path (Path | str): The file that needs to be handled.\n\n        Returns:\n            dict[str, Handler]: The handlers associated with this file type.\n        \"\"\"\n        file_path = Path(file_path)\n        file_ext = file_path.suffix.lower()\n\n        return {\n            handle_name: FileProcessor.get_handler(handle_name)\n            for handle_name in FileProcessor._handlers.get(file_ext, [])\n        }\n\n    @staticmethod\n    def parse_file(file_path: Path | str) -&gt; ParsedDocumentType:\n        \"\"\"Parses a file and returns the `ParsedDocument` after retrieving the 'main' handler for it.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            ParsedDocumentType: The parsed format of the file.\n        \"\"\"\n        handler = FileProcessor.get_main_handler(file_path)\n        return handler.parse_file(file_path)\n\n    @staticmethod\n    def process_file(file_path: Path | str, **kwargs) -&gt; Path:\n        \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it\n        after retrieving the 'main' handler for it.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n        \"\"\"\n        handler = FileProcessor.get_main_handler(file_path)\n        return handler.process_file(file_path, **kwargs)\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.get_handler","title":"<code>get_handler(handler_name)</code>  <code>staticmethod</code>","text":"<p>Retrieve the registered handler from the given <code>handler_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>handler_name</code> <code>str</code> <p>Unique name given to the registered <code>Handler</code>.</p> required <p>Returns:</p> Name Type Description <code>Handler</code> <code>Handler</code> <p>The registered <code>Handler</code>.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_handler(handler_name: str) -&gt; Handler:\n    \"\"\"Retrieve the registered handler from the given `handler_name`.\n\n    Args:\n        handler_name (str): Unique name given to the registered `Handler`.\n\n    Returns:\n        Handler: The registered `Handler`.\n    \"\"\"\n    return FileProcessor._registered_handlers[handler_name]\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.get_handlers","title":"<code>get_handlers(file_path)</code>  <code>staticmethod</code>","text":"<p>Get all the handlers that can handle the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file that needs to be handled.</p> required <p>Returns:</p> Type Description <code>dict[str, Handler]</code> <p>dict[str, Handler]: The handlers associated with this file type.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_handlers(file_path: Path | str) -&gt; dict[str, Handler]:\n    \"\"\"Get all the handlers that can handle the given file.\n\n    Args:\n        file_path (Path | str): The file that needs to be handled.\n\n    Returns:\n        dict[str, Handler]: The handlers associated with this file type.\n    \"\"\"\n    file_path = Path(file_path)\n    file_ext = file_path.suffix.lower()\n\n    return {\n        handle_name: FileProcessor.get_handler(handle_name)\n        for handle_name in FileProcessor._handlers.get(file_ext, [])\n    }\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.get_main_handler","title":"<code>get_main_handler(file_path)</code>  <code>staticmethod</code>","text":"<p>Get the 'main' handler that can handle the given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file that needs to be handled.</p> required <p>Returns:</p> Name Type Description <code>Handler</code> <code>Handler</code> <p>The main handler associated with this file type.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef get_main_handler(file_path: Path | str) -&gt; Handler:\n    \"\"\"Get the 'main' handler that can handle the given file.\n\n    Args:\n        file_path (Path | str): The file that needs to be handled.\n\n    Returns:\n        Handler: The main handler associated with this file type.\n    \"\"\"\n    file_path = Path(file_path)\n    file_ext = file_path.suffix.lower()\n\n    handler_id = FileProcessor._main_handlers.get(file_ext, None)\n    if handler_id is None:\n        raise ValueError(f\"No main handler registered for file type: {file_ext}\")\n\n    return FileProcessor._registered_handlers[handler_id]\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.parse_file","title":"<code>parse_file(file_path)</code>  <code>staticmethod</code>","text":"<p>Parses a file and returns the <code>ParsedDocument</code> after retrieving the 'main' handler for it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>The parsed format of the file.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef parse_file(file_path: Path | str) -&gt; ParsedDocumentType:\n    \"\"\"Parses a file and returns the `ParsedDocument` after retrieving the 'main' handler for it.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        ParsedDocumentType: The parsed format of the file.\n    \"\"\"\n    handler = FileProcessor.get_main_handler(file_path)\n    return handler.parse_file(file_path)\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.process_file","title":"<code>process_file(file_path, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Parses a file and saves the <code>ParsedDocument</code> json and returns the file path to it after retrieving the 'main' handler for it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the json exported <code>ParsedDocument</code> which is nearby the given <code>file_path</code>.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef process_file(file_path: Path | str, **kwargs) -&gt; Path:\n    \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it\n    after retrieving the 'main' handler for it.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n    \"\"\"\n    handler = FileProcessor.get_main_handler(file_path)\n    return handler.process_file(file_path, **kwargs)\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.FileProcessor.register","title":"<code>register(extension, *, converters=None, main=False)</code>  <code>staticmethod</code>","text":"<p>A <code>register</code> decorator that registers a <code>Parser</code> to specified document <code>extension</code> type and the list of <code>Converter</code>s that needs to run beforing parsing the document.</p> <p>Example:</p> <pre><code>@FileProcessor.register(\".html\")\n@FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n@FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\nclass HTMLParser(Parser):\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension that the parser will handle.</p> required <code>converters</code> <code>type[Converter] | Iterable[type[Converter]] | None</code> <p>List of <code>Converter</code>s that converts a given file first before parsing it. Defaults to None.</p> <code>None</code> <code>main</code> <code>bool</code> <p>Specifies if the decorated <code>Parser</code> is the 'main' parser for this extension type. You can have multiple parsers for the same extension but only one of them can be the 'main' one. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[P], P]</code> <p>Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@staticmethod\ndef register(\n    extension: str, *, converters: type[Converter] | Iterable[type[Converter]] | None = None, main: bool = False\n) -&gt; Callable[[P], P]:\n    \"\"\"A `register` decorator that registers a `Parser` to specified document `extension` type\n    and the list of `Converter`s that needs to run beforing parsing the document.\n\n    Example:\n\n    ```python\n    @FileProcessor.register(\".html\")\n    @FileProcessor.register(\".pdf\", converters=PDF2HTMLConverter, main=True)\n    @FileProcessor.register(\".docx\", converters=[Docx2PDFConverter, PDF2HTMLConverter])\n    class HTMLParser(Parser):\n        def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n            ...\n    ```\n\n    Args:\n        extension (str): The file extension that the parser will handle.\n        converters (type[Converter] | Iterable[type[Converter]] | None, optional):\n            List of `Converter`s that converts a given file first before parsing it. Defaults to None.\n        main (bool, optional): Specifies if the decorated `Parser` is the 'main' parser for this extension type.\n            You can have multiple parsers for the same extension but only one of them can be the 'main' one.\n            Defaults to False.\n\n    Returns:\n        Callable[[type[Parser]], type[Parser]]: A decorator that registers the given Parser\n    \"\"\"\n    # NOTE: this only works if the register has reached. Meaning we need to manually import it in __init__.py\n    extension = extension.lower()\n    if not extension.startswith(\".\"):\n        extension = f\".{extension}\"\n    if not FileProcessor._file_extension_pattern.match(extension):\n        raise ValueError(f\"Invalid file extension: {extension}\")\n    if converters is not None and not isinstance(converters, Iterable):\n        converters = (converters,)\n    if converters is not None and (\n        not_converters := [converter for converter in converters if not issubclass(converter, Converter)]\n    ):\n        raise TypeError(f\"{not_converters} are not subclasses of {Converter}\")\n    if main and extension in FileProcessor._main_handlers:\n        raise ValueError(\n            f\"{extension} is already registered to {FileProcessor._main_handlers[extension]}! \"\n            \"Only one main parser can be registered for given extension.\"\n        )\n\n    def decorator(parser: P) -&gt; P:\n        if not issubclass(parser, Parser):\n            raise TypeError(f\"{parser} is not a subclass of {Parser}\")\n        handler = Handler(\n            parser=parser(), converters=tuple(converter() for converter in converters) if converters else None\n        )\n        FileProcessor._register(handler, extension, main=main)\n        return parser\n\n    return decorator\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.Handler","title":"<code>Handler</code>  <code>dataclass</code>","text":"<p>Handler saves combination of parser and it's required <code>Converter</code>s to process a given file document.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>@dataclass\nclass Handler:\n    \"\"\"\n    Handler saves combination of parser and it's required `Converter`s to process a given file document.\n    \"\"\"\n\n    parser: Parser\n    converters: tuple[Converter, ...] | None = None\n\n    @property\n    def name(self) -&gt; str:\n        name = f\"{type(self.parser).__name__}\"\n        if self.converters:\n            name += f'[{\"|\".join((type(converter).__name__ for converter in self.converters))}]'\n        return name\n\n    def convert(self, file_path: Path) -&gt; Path:\n        \"\"\"Takes a file_path and chains all the converters ont that file,\n         saves it and returns the location of the converted file.\n\n        Args:\n            file_path (Path): The file_path that needs to be converted.\n\n        Returns:\n            Path: The path to the converted file.\n        \"\"\"\n        return reduce(lambda r, f: f.convert(r), self.converters, file_path) if self.converters else file_path\n\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        \"\"\"Parses the given file_path by the given `Parser` after being converted by the given `Converter`s.\n\n        Args:\n            file_path (Path): The file_path that needs to be parsed.\n\n        Returns:\n            ParsedDocumentType: The parsed format of the file.\n        \"\"\"\n        return self.parser.parse(self.convert(file_path))\n\n    def parse_file(self, file_path: Path | str) -&gt; ParsedDocumentType:\n        \"\"\"Parses a file and returns the `ParsedDocument`.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            ParsedDocumentType: The parsed format of the file.\n        \"\"\"\n        file_path = Path(file_path)\n        parsed_document = self.parse(file_path)\n        return parsed_document\n\n    def process_file(self, file_path: Path | str, **kwargs) -&gt; Path:\n        \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it.\n\n        Args:\n            file_path (Path | str): The file_path that needs to be parsed.\n\n        Returns:\n            Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n        \"\"\"\n        file_path = Path(file_path)\n        file_ext = file_path.suffix.lower()\n        parsed_document = self.parse_file(file_path)\n        parsed_out_file = file_path.with_suffix(f\"{file_ext}.json\")\n        export_parsed_document(parsed_document, parsed_out_file, **kwargs)\n        return parsed_out_file\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.Handler.convert","title":"<code>convert(file_path)</code>","text":"<p>Takes a file_path and chains all the converters ont that file,  saves it and returns the location of the converted file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The file_path that needs to be converted.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the converted file.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>def convert(self, file_path: Path) -&gt; Path:\n    \"\"\"Takes a file_path and chains all the converters ont that file,\n     saves it and returns the location of the converted file.\n\n    Args:\n        file_path (Path): The file_path that needs to be converted.\n\n    Returns:\n        Path: The path to the converted file.\n    \"\"\"\n    return reduce(lambda r, f: f.convert(r), self.converters, file_path) if self.converters else file_path\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.Handler.parse","title":"<code>parse(file_path)</code>","text":"<p>Parses the given file_path by the given <code>Parser</code> after being converted by the given <code>Converter</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>The parsed format of the file.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n    \"\"\"Parses the given file_path by the given `Parser` after being converted by the given `Converter`s.\n\n    Args:\n        file_path (Path): The file_path that needs to be parsed.\n\n    Returns:\n        ParsedDocumentType: The parsed format of the file.\n    \"\"\"\n    return self.parser.parse(self.convert(file_path))\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.Handler.parse_file","title":"<code>parse_file(file_path)</code>","text":"<p>Parses a file and returns the <code>ParsedDocument</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>The parsed format of the file.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>def parse_file(self, file_path: Path | str) -&gt; ParsedDocumentType:\n    \"\"\"Parses a file and returns the `ParsedDocument`.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        ParsedDocumentType: The parsed format of the file.\n    \"\"\"\n    file_path = Path(file_path)\n    parsed_document = self.parse(file_path)\n    return parsed_document\n</code></pre>"},{"location":"reference/core/parser/file_processor/#supermat.core.parser.file_processor.Handler.process_file","title":"<code>process_file(file_path, **kwargs)</code>","text":"<p>Parses a file and saves the <code>ParsedDocument</code> json and returns the file path to it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>The file_path that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The path to the json exported <code>ParsedDocument</code> which is nearby the given <code>file_path</code>.</p> Source code in <code>supermat/core/parser/file_processor.py</code> <pre><code>def process_file(self, file_path: Path | str, **kwargs) -&gt; Path:\n    \"\"\"Parses a file and saves the `ParsedDocument` json and returns the file path to it.\n\n    Args:\n        file_path (Path | str): The file_path that needs to be parsed.\n\n    Returns:\n        Path: The path to the json exported `ParsedDocument` which is nearby the given `file_path`.\n    \"\"\"\n    file_path = Path(file_path)\n    file_ext = file_path.suffix.lower()\n    parsed_document = self.parse_file(file_path)\n    parsed_out_file = file_path.with_suffix(f\"{file_ext}.json\")\n    export_parsed_document(parsed_document, parsed_out_file, **kwargs)\n    return parsed_out_file\n</code></pre>"},{"location":"reference/core/parser/utils/","title":"Utils","text":""},{"location":"reference/core/parser/utils/#supermat.core.parser.utils.extract_meaningful_words","title":"<code>extract_meaningful_words(text)</code>","text":"<p>For given text, extract set of relevant keywords using nltk.</p> Source code in <code>supermat/core/parser/utils.py</code> <pre><code>def extract_meaningful_words(text: str) -&gt; set[str]:\n    \"\"\"For given text, extract set of relevant keywords using nltk.\"\"\"\n    # Tokenize the sentence\n    tokens = nltk_word_tokenize(text)\n    # Perform POS tagging\n    tagged_tokens = nltk_pos_tag(tokens)\n    # Extract words with more than 4 characters, numerics, nouns, verbs, adverbs, and adjectives excluding pronouns\n    keywords = [\n        word\n        for word, tag in tagged_tokens\n        if ((tag.startswith((\"NN\", \"VB\", \"JJ\", \"RB\")) and len(word) &gt; 4) or (tag == \"CD\")) and word.lower() != \"i\"\n    ]\n    return set(keywords)\n</code></pre>"},{"location":"reference/core/parser/utils/#supermat.core.parser.utils.get_keywords","title":"<code>get_keywords(text)</code>","text":"<p>For given text, retrieve relevant list of keywords using spacy and nltk.</p> Source code in <code>supermat/core/parser/utils.py</code> <pre><code>def get_keywords(text: str) -&gt; list[str]:\n    \"\"\"For given text, retrieve relevant list of keywords using spacy and nltk.\"\"\"\n    return list(extract_spacy_keywords(text) | extract_meaningful_words(text))\n</code></pre>"},{"location":"reference/core/parser/utils/#supermat.core.parser.utils.split_text_into_token_chunks","title":"<code>split_text_into_token_chunks(text, max_tokens=8000, model_name=TOKENIZER_MODEL_NAME)</code>","text":"<p>Splits a text into chunks based on token count using LangChain's token splitter.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be split.</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens in each chunk.</p> <code>8000</code> <code>model_name</code> <code>str</code> <p>The LLM model name to determine tokenization rules.</p> <code>TOKENIZER_MODEL_NAME</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>A list of text chunks, each with up to max_tokens tokens.</p> Source code in <code>supermat/core/parser/utils.py</code> <pre><code>def split_text_into_token_chunks(text, max_tokens: int = 8000, model_name: str = TOKENIZER_MODEL_NAME) -&gt; list[str]:\n    \"\"\"\n    Splits a text into chunks based on token count using LangChain's token splitter.\n\n    Args:\n        text (str): The text to be split.\n        max_tokens (int): The maximum number of tokens in each chunk.\n        model_name (str): The LLM model name to determine tokenization rules.\n\n    Returns:\n        list: A list of text chunks, each with up to max_tokens tokens.\n    \"\"\"\n    encoding = tiktoken.encoding_for_model(model_name)\n    splitter = TokenTextSplitter(encoding_name=encoding.name, chunk_size=max_tokens, chunk_overlap=0)\n    chunks = splitter.split_text(text)\n    return chunks\n</code></pre>"},{"location":"reference/core/parser/adobe_parser/","title":"Adobe parser","text":"<p>Parsing pdf files using Adobe PDF Services We have a pydantic model that best represents the processed json file from adobe which makes it easier to parse.</p>"},{"location":"reference/core/parser/adobe_parser/_adobe_doc_cacher/","title":"adobe doc cacher","text":"<p>Since adobe is a paid service, we cache all the outputs of a given pdf file that adobe provides to avoid calling the api</p>"},{"location":"reference/core/parser/adobe_parser/_adobe_doc_cacher/#supermat.core.parser.adobe_parser._adobe_doc_cacher.CachedFile","title":"<code>CachedFile</code>","text":"<p>A singleton Cache mechanism that caches a given pdf file. The cached index contains the mapping of the original pdf file path to the processed adobe zip file.</p> Source code in <code>supermat/core/parser/adobe_parser/_adobe_doc_cacher.py</code> <pre><code>class CachedFile:\n    \"\"\"\n    A singleton Cache mechanism that caches a given pdf file.\n    The cached index contains the mapping of the original pdf file path to the processed adobe zip file.\n    \"\"\"\n\n    _instance: Self | None = None\n    _lock = threading.Lock()\n    _cache_dir: Path | None = None\n    _cache_index: dict[Path, CacheIndex] = {}\n    _cache_index_filename = \"__cache_index.json\"\n    max_cache_size = 100\n\n    def __new__(cls):\n        CachedFile._setup_tmp_dir()\n        if not cls._instance:\n            with cls._lock:\n                if not cls._instance:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n\n    @staticmethod\n    def _setup_tmp_dir():\n        if CachedFile._cache_dir is None:\n            CachedFile._cache_dir = get_persistent_temp_directory()\n            CachedFile.reload_cache_index()\n\n    def __init__(self):\n        assert CachedFile._cache_dir is not None\n        self._cache_path = CachedFile._cache_dir\n\n    def exists(self, pdf_file: Path) -&gt; bool:\n        return pdf_file in CachedFile._cache_index\n\n    def get_cached_file_path(self, pdf_file: Path) -&gt; Path:\n        return CachedFile._cache_index[pdf_file][\"cached_file_path\"]\n\n    @staticmethod\n    def get_cache_index_path() -&gt; Path | None:\n        if CachedFile._cache_dir is None:\n            return None\n        return Path(CachedFile._cache_dir) / CachedFile._cache_index_filename\n\n    @staticmethod\n    def update_cache_file():\n        cache_index_path = CachedFile.get_cache_index_path()\n        if cache_index_path is None:\n            return\n\n        with cache_index_path.open(\"wb+\") as fp:\n            fp.write(\n                orjson.dumps({k.as_posix(): v for k, v in CachedFile._cache_index.items()}, default=orjson_defaults)\n            )\n\n    @staticmethod\n    def reload_cache_index():\n        cache_index_path = CachedFile.get_cache_index_path()\n        if cache_index_path is not None and cache_index_path.exists():\n            with cache_index_path.open(\"rb\") as fp:\n                CachedFile._cache_index = {\n                    Path(path): CacheIndex(**cache_index) for path, cache_index in orjson.loads(fp.read()).items()\n                }\n\n    def create_file(self, pdf_file: Path, suffix: str = \".zip\") -&gt; Path:\n        cache_index = CacheIndex(\n            timestamp=datetime.now().timestamp(),\n            cached_file_path=self._cache_path / pdf_file.with_suffix(pdf_file.suffix + suffix).name,\n        )\n        CachedFile._cache_index[pdf_file] = cache_index\n        CachedFile.cleanup_cache()\n        CachedFile.update_cache_file()\n        return self.get_cached_file_path(pdf_file)\n\n    @staticmethod\n    def cleanup_cache():\n        if len(CachedFile._cache_index) &gt; CachedFile.max_cache_size:\n            oldest = sorted(CachedFile._cache_index.items(), key=lambda x: x[1][\"timestamp\"])[0]\n            del_cache_index = CachedFile._cache_index.pop(oldest[0])\n            del_cache_index[\"cached_file_path\"].unlink(missing_ok=True)\n\n    @staticmethod\n    def clear_index():\n        CachedFile._cache_index.clear()\n\n    @staticmethod\n    def cleanup():\n        if CachedFile._cache_dir:\n            shutil.rmtree(CachedFile._cache_dir)\n            CachedFile._cache_dir = get_persistent_temp_directory()\n</code></pre>"},{"location":"reference/core/parser/adobe_parser/adobe_internal_model/","title":"Adobe internal model","text":""},{"location":"reference/core/parser/adobe_parser/parser/","title":"Parser","text":""},{"location":"reference/core/parser/pymupdf_parser/","title":"Pymupdf parser","text":"<p>Parsing pdf files using the popular open source PyMuPDF parser.</p>"},{"location":"reference/core/parser/pymupdf_parser/#supermat.core.parser.pymupdf_parser.PyMuPDFParser","title":"<code>PyMuPDFParser</code>","text":"<p>               Bases: <code>Parser</code></p> <p>Parses a pdf file using PyMuPDF library.</p> Source code in <code>supermat/core/parser/pymupdf_parser/parser.py</code> <pre><code>@FileProcessor.register(\".pdf\")\nclass PyMuPDFParser(Parser):\n    \"\"\"Parses a pdf file using PyMuPDF library.\"\"\"\n\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        parsed_pdf = parse_pdf(file_path)\n        return process_pymupdf(parsed_pdf)\n</code></pre>"},{"location":"reference/core/parser/pymupdf_parser/parser/","title":"Parser","text":""},{"location":"reference/core/parser/pymupdf_parser/parser/#supermat.core.parser.pymupdf_parser.parser.PyMuPDFParser","title":"<code>PyMuPDFParser</code>","text":"<p>               Bases: <code>Parser</code></p> <p>Parses a pdf file using PyMuPDF library.</p> Source code in <code>supermat/core/parser/pymupdf_parser/parser.py</code> <pre><code>@FileProcessor.register(\".pdf\")\nclass PyMuPDFParser(Parser):\n    \"\"\"Parses a pdf file using PyMuPDF library.\"\"\"\n\n    def parse(self, file_path: Path) -&gt; ParsedDocumentType:\n        parsed_pdf = parse_pdf(file_path)\n        return process_pymupdf(parsed_pdf)\n</code></pre>"},{"location":"reference/core/parser/pymupdf_parser/parser/#supermat.core.parser.pymupdf_parser.parser.get_path","title":"<code>get_path(*args)</code>","text":"<p>Create path from page number, block number and line number.</p> Source code in <code>supermat/core/parser/pymupdf_parser/parser.py</code> <pre><code>def get_path(*args: int) -&gt; str:\n    \"\"\"Create path from page number, block number and line number.\"\"\"\n    return \"/\".join(map(str, args))\n</code></pre>"},{"location":"reference/core/parser/pymupdf_parser/parser/#supermat.core.parser.pymupdf_parser.parser.process_pymupdf","title":"<code>process_pymupdf(parsed_pdf)</code>","text":"<p>Converts a pdf, page and by page, and block by block using PyMuPDF.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_pdf</code> <code>PyMuPDFDocument</code> <p>Pydantic model representation of pymupdf document.</p> required <p>Returns:</p> Name Type Description <code>ParsedDocumentType</code> <code>ParsedDocumentType</code> <p>Parsed form of the pdf.</p> Source code in <code>supermat/core/parser/pymupdf_parser/parser.py</code> <pre><code>def process_pymupdf(parsed_pdf: PyMuPDFDocument) -&gt; ParsedDocumentType:\n    \"\"\"Converts a pdf, page and by page, and block by block using PyMuPDF.\n\n    Args:\n        parsed_pdf (PyMuPDFDocument): Pydantic model representation of pymupdf document.\n\n    Returns:\n        ParsedDocumentType: Parsed form of the pdf.\n    \"\"\"\n    chunks = []\n    for page in parsed_pdf.pages:\n        for block in page.blocks:\n            if isinstance(block, TextBlock):\n                # TODO (@legendof-selda): create keys\n                sentence_chunks: list[TextChunk] = []\n                for line_no, line in enumerate(block.lines):\n                    # NOTE: we are combining spans to lines as spans are not very relevant.\n                    # All lines have a signle span\n                    line_text = \"\".join(span.text for span in line.spans)\n                    # NOTE: we are assuming the font properties to be the same for all spans in a line.\n                    # This can lead to losing certain info like underline, bold, etc for small parts of texts.\n                    first_span = line.spans[0]\n\n                    structure = get_structure(page.number + 1, block.number + 1, line_no + 1)\n\n                    sentence_chunk = TextChunk(\n                        structure=structure,\n                        properties=TextChunkProperty(\n                            font=FontProperties(\n                                name=first_span.font, **first_span.model_dump(exclude={\"text\", \"bbox\"})\n                            ),\n                            text_size=first_span.size,\n                            bounds=line.bbox,\n                            page=page.number,\n                            path=get_path(page.number, block.number, line_no),\n                            # TODO (@legendof-selda): need to figure out a way to get attributes if possible\n                        ),\n                        text=line_text,\n                        key=get_keywords(line_text),\n                    )\n                    sentence_chunks.append(sentence_chunk)\n\n                if TYPE_CHECKING:\n                    assert sentence_chunks[0].properties\n\n                text = \" \".join(sentence_chunk.text for sentence_chunk in sentence_chunks)\n                chunk = TextChunk(\n                    structure=get_structure(page.number + 1, block.number + 1),\n                    text=text,\n                    key=get_keywords(text),\n                    sentences=sentence_chunks if len(sentence_chunks) &gt; 1 else None,\n                    properties=(\n                        None\n                        if len(sentence_chunks) &lt;= 1\n                        else TextChunkProperty(\n                            **(\n                                sentence_chunks[0].properties.model_dump(exclude={\"Path\", \"Bounds\"})\n                                | {\"Path\": get_path(page.number, block.number), \"Bounds\": block.bbox}\n                            )\n                        )\n                    ),\n                )\n            elif isinstance(block, ImageBlock):\n                chunk = ImageChunk(\n                    structure=get_structure(page.number + 1, block.number + 1),\n                    bounds=block.bbox,\n                    page=page.number,\n                    path=get_path(page.number, block.number),\n                    figure_object=block.image,\n                    attributes=block.model_dump(exclude={\"number\", \"type_\", \"bbox\", \"image\"}),\n                )\n            else:\n                # NOTE: Need to figure out how to get Footnote type\n                raise ValueError(f\"Invalid block type {block.type_}\")\n\n            chunks.append(chunk)\n\n    return ParsedDocument.validate_python(chunks)\n</code></pre>"},{"location":"reference/core/parser/pymupdf_parser/pymupdf_internal_model/","title":"Pymupdf internal model","text":""},{"location":"reference/core/parser/pymupdf_parser/utils/","title":"Utils","text":""},{"location":"reference/core/parser/pymupdf_parser/utils/#supermat.core.parser.pymupdf_parser.utils.parse_pdf","title":"<code>parse_pdf(pdf_file)</code>","text":"<p>Converts pdf file to a PyMuPDF Document model to easy parsing. pymupdf, doesn't provide a pydantic model in their implementation. We convert it into a pydantic model to make it easier to work with.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_file</code> <code>Path</code> <p>The pdf file that needs to be parsed.</p> required <p>Returns:</p> Name Type Description <code>PyMuPDFDocument</code> <code>PyMuPDFDocument</code> <p>Pydantic model representation of the pdf file.</p> Source code in <code>supermat/core/parser/pymupdf_parser/utils.py</code> <pre><code>def parse_pdf(pdf_file: Path) -&gt; PyMuPDFDocument:\n    \"\"\"Converts pdf file to a PyMuPDF Document model to easy parsing.\n    pymupdf, doesn't provide a pydantic model in their implementation.\n    We convert it into a pydantic model to make it easier to work with.\n\n    Args:\n        pdf_file (Path): The pdf file that needs to be parsed.\n\n    Returns:\n        PyMuPDFDocument: Pydantic model representation of the pdf file.\n    \"\"\"\n    doc = pymupdf.open(pdf_file)\n    doc_data = {\"filename\": pdf_file.name, \"total_pages\": len(doc), \"pages\": [create_page(page) for page in doc]}\n    return PyMuPDFDocument.model_validate_json(orjson.dumps(doc_data, default=default))\n</code></pre>"},{"location":"reference/gradio/","title":"Gradio","text":""},{"location":"reference/gradio/#supermat.gradio.LLMChat","title":"<code>LLMChat</code>","text":"Source code in <code>supermat/gradio/__init__.py</code> <pre><code>class LLMChat:\n    def __init__(self):\n        self.chat_model = None\n        self.retriever = None\n\n    def initialize_client(\n        self,\n        provider: str,\n        model: str,\n        credentials: str | None = None,\n        base_url: str | None = None,\n        temperature: float | None = 0.0,\n    ) -&gt; str:\n        \"\"\"Initialize the LangChain chat model based on selected provider.\"\"\"\n        self.provider = provider\n        self.model = model\n\n        if TYPE_CHECKING:\n            assert isinstance(credentials, SecretStr)\n\n        try:\n            match (provider):\n                case LLMProvider.ollama:\n                    from langchain_ollama.llms import OllamaLLM  # noqa: I900\n\n                    self.chat_model = OllamaLLM(model=model, temperature=temperature, base_url=base_url)\n                case LLMProvider.anthropic:\n                    from langchain_anthropic import ChatAnthropic  # noqa: I900\n\n                    self.chat_model = ChatAnthropic(\n                        model_name=model, temperature=temperature, timeout=None, api_key=credentials, stop=None\n                    )\n                case LLMProvider.openai:\n                    from langchain_openai import ChatOpenAI  # noqa: I900\n\n                    temperature = 0.7 if temperature is None else temperature\n                    self.chat_model = ChatOpenAI(model=model, temperature=temperature, api_key=credentials)\n                case _:\n                    return f\"Invalid LLM Provider {provider}\"\n\n            return f\"{self.chat_model.get_name()} initialized successfully!\"\n\n        except Exception as e:\n            return f\"Error initializing client: {str(e)}\"\n\n    def parse_files(self, collection_name: str, pdf_files: Sequence[Path | str]) -&gt; str:\n        pdf_files = list(map(Path, pdf_files))\n        if TYPE_CHECKING:\n            pdf_files = cast(list[Path], pdf_files)\n\n        if not all(f.exists() for f in pdf_files):\n            return \"Few files do not exist.\"\n        non_pdf_files = [f.name for f in pdf_files if f.suffix.lower() != \".pdf\"]\n        if non_pdf_files:\n            return f\"Following files are not pdf: \\n{'\\n'.join(non_pdf_files)}\"\n\n        parsed_files = Parallel(n_jobs=-1, backend=\"threading\")(\n            delayed(FileProcessor.parse_file)(path) for path in pdf_files\n        )\n\n        if TYPE_CHECKING:\n            parsed_files = cast(list[ParsedDocumentType], parsed_files)\n\n        documents = list(chain.from_iterable(parsed_docs for parsed_docs in parsed_files))\n\n        if TYPE_CHECKING:\n            documents = cast(ParsedDocumentType, documents)\n\n        retriever = SupermatRetriever(\n            parsed_docs=documents,\n            vector_store=Chroma(\n                embedding_function=HuggingFaceEmbeddings(\n                    model_name=\"thenlper/gte-base\",\n                ),\n                persist_directory=\"./chromadb\",\n                collection_name=collection_name,\n            ),\n        )\n        self.retriever = retriever\n        return \"Files parsed successfully.\"\n\n    def convert_history_to_messages(self, history: list[dict]) -&gt; list[HumanMessage | AIMessage]:\n        \"\"\"Convert Gradio chat history to LangChain message format.\"\"\"\n\n        return [\n            HumanMessage(content=msg[\"content\"]) if msg[\"role\"] == \"user\" else AIMessage(content=msg[\"content\"])\n            for msg in history\n        ]\n\n    @property\n    def chain(self) -&gt; RunnableSerializable:\n        assert self.chat_model and self.retriever\n        chain = get_default_chain(self.retriever, self.chat_model, substitute_references=False, return_context=False)\n        return chain\n\n    def chat(self, message: str, _history):\n        \"\"\"Process chat message using LangChain chat model.\"\"\"\n        if not self.chat_model:\n            return \"Please initialize an LLM provider first!\"\n\n        if not self.retriever:\n            return \"Please parse relevant pdf documents!\"\n\n        try:\n            # history_langchain_format = self.convert_history_to_messages(history)\n            # history_langchain_format.append(HumanMessage(content=message))\n            gpt_response = self.chain.invoke(message)\n            return gpt_response if isinstance(gpt_response, str) else gpt_response.content\n\n        except Exception as e:\n            return f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n</code></pre>"},{"location":"reference/gradio/#supermat.gradio.LLMChat.chat","title":"<code>chat(message, _history)</code>","text":"<p>Process chat message using LangChain chat model.</p> Source code in <code>supermat/gradio/__init__.py</code> <pre><code>def chat(self, message: str, _history):\n    \"\"\"Process chat message using LangChain chat model.\"\"\"\n    if not self.chat_model:\n        return \"Please initialize an LLM provider first!\"\n\n    if not self.retriever:\n        return \"Please parse relevant pdf documents!\"\n\n    try:\n        # history_langchain_format = self.convert_history_to_messages(history)\n        # history_langchain_format.append(HumanMessage(content=message))\n        gpt_response = self.chain.invoke(message)\n        return gpt_response if isinstance(gpt_response, str) else gpt_response.content\n\n    except Exception as e:\n        return f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n</code></pre>"},{"location":"reference/gradio/#supermat.gradio.LLMChat.convert_history_to_messages","title":"<code>convert_history_to_messages(history)</code>","text":"<p>Convert Gradio chat history to LangChain message format.</p> Source code in <code>supermat/gradio/__init__.py</code> <pre><code>def convert_history_to_messages(self, history: list[dict]) -&gt; list[HumanMessage | AIMessage]:\n    \"\"\"Convert Gradio chat history to LangChain message format.\"\"\"\n\n    return [\n        HumanMessage(content=msg[\"content\"]) if msg[\"role\"] == \"user\" else AIMessage(content=msg[\"content\"])\n        for msg in history\n    ]\n</code></pre>"},{"location":"reference/gradio/#supermat.gradio.LLMChat.initialize_client","title":"<code>initialize_client(provider, model, credentials=None, base_url=None, temperature=0.0)</code>","text":"<p>Initialize the LangChain chat model based on selected provider.</p> Source code in <code>supermat/gradio/__init__.py</code> <pre><code>def initialize_client(\n    self,\n    provider: str,\n    model: str,\n    credentials: str | None = None,\n    base_url: str | None = None,\n    temperature: float | None = 0.0,\n) -&gt; str:\n    \"\"\"Initialize the LangChain chat model based on selected provider.\"\"\"\n    self.provider = provider\n    self.model = model\n\n    if TYPE_CHECKING:\n        assert isinstance(credentials, SecretStr)\n\n    try:\n        match (provider):\n            case LLMProvider.ollama:\n                from langchain_ollama.llms import OllamaLLM  # noqa: I900\n\n                self.chat_model = OllamaLLM(model=model, temperature=temperature, base_url=base_url)\n            case LLMProvider.anthropic:\n                from langchain_anthropic import ChatAnthropic  # noqa: I900\n\n                self.chat_model = ChatAnthropic(\n                    model_name=model, temperature=temperature, timeout=None, api_key=credentials, stop=None\n                )\n            case LLMProvider.openai:\n                from langchain_openai import ChatOpenAI  # noqa: I900\n\n                temperature = 0.7 if temperature is None else temperature\n                self.chat_model = ChatOpenAI(model=model, temperature=temperature, api_key=credentials)\n            case _:\n                return f\"Invalid LLM Provider {provider}\"\n\n        return f\"{self.chat_model.get_name()} initialized successfully!\"\n\n    except Exception as e:\n        return f\"Error initializing client: {str(e)}\"\n</code></pre>"},{"location":"reference/langchain/","title":"Langchain","text":"<p>Langchain bindings on the supermat core module. The SupermatRetriever is a drop in replacement for Langchain VectorStore.</p>"},{"location":"reference/langchain/#supermat.langchain.SupermatRetriever","title":"<code>SupermatRetriever</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Supermat Langchain Custom Retriever. This uses any Langchain VectorStore and overrides the documents retrieval methods to make it work for Supermat. NOTE: Currently this only works on Text chunks.</p> <pre><code>from supermat.langchain.bindings import SupermatRetriever\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nretriever = SupermatRetriever(\n    parsed_docs=FileProcessor.process_file(pdf_file_path),\n    document_name=pdf_file_path.stem,\n    vector_store=Chroma(\n        embedding_function=HuggingFaceEmbeddings(\n            model_name=\"thenlper/gte-base\",\n        )\n    ),\n)\n</code></pre> <p>Args:     parsed_docs (ParsedDocumentType): The supermat parsed documents.     vector_store (VectorStore): The vector store used to store the document chunks.     vector_store_retriver_kwargs (dict[str, Any], optional): <code>VectorStore</code> kwargs used during initialization.         Defaults to <code>{}</code>.     max_chunk_length (int, optional): Max character length. NOTE: This needs to be based on tokens instead.         Defaults to 8000.     store_sentences (bool, optional): Store sentence level chunks in vector store         which will then be converted to paragraphs before sending to LLM. Defaults to False.</p> Source code in <code>supermat/langchain/bindings.py</code> <pre><code>class SupermatRetriever(BaseRetriever):\n    \"\"\"\n    Supermat Langchain Custom Retriever.\n    This uses any Langchain VectorStore and overrides the documents retrieval methods to make it work for Supermat.\n    NOTE: Currently this only works on Text chunks.\n\n\n    ``` python\n    from supermat.langchain.bindings import SupermatRetriever\n    from langchain_chroma import Chroma\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    retriever = SupermatRetriever(\n        parsed_docs=FileProcessor.process_file(pdf_file_path),\n        document_name=pdf_file_path.stem,\n        vector_store=Chroma(\n            embedding_function=HuggingFaceEmbeddings(\n                model_name=\"thenlper/gte-base\",\n            )\n        ),\n    )\n    ```\n    Args:\n        parsed_docs (ParsedDocumentType): The supermat parsed documents.\n        vector_store (VectorStore): The vector store used to store the document chunks.\n        vector_store_retriver_kwargs (dict[str, Any], optional): `VectorStore` kwargs used during initialization.\n            Defaults to `{}`.\n        max_chunk_length (int, optional): Max character length. NOTE: This needs to be based on tokens instead.\n            Defaults to 8000.\n        store_sentences (bool, optional): Store sentence level chunks in vector store\n            which will then be converted to paragraphs before sending to LLM. Defaults to False.\n    \"\"\"\n\n    parsed_docs: ParsedDocumentType = Field(exclude=True, strict=False, repr=False)\n    vector_store: VectorStore\n    vector_store_retriver_kwargs: dict[str, Any] = {}\n    max_chunk_length: int = 8000\n    store_sentences: bool = False\n\n    @cached_property\n    def vector_store_retriver(self) -&gt; VectorStoreRetriever:\n        return self.vector_store.as_retriever(**self.vector_store_retriver_kwargs)\n\n    def _create_document_index(self) -&gt; tuple[dict[str, int], dict[int, str]]:\n        documents = {\n            chunk.document\n            for chunk in self.parsed_docs\n            # NOTE: we assume that all chunks have document\n            if chunk.document is not None\n        }\n        # NOTE: we want the document id to start with 1, since 0 means all in structure id.\n        document_index_map = {document: doc_id for doc_id, document in enumerate(documents, 1)}\n        index_document_map = dict(zip(document_index_map.values(), document_index_map.keys()))\n        return document_index_map, index_document_map\n\n    def _add_doc_id(self, document_index_map: dict[str, int]):\n        \"\"\"\n        Mutates current `parsed_docs` to include document id in the chunk structure id.\n        This is a temporary solution.\n        Currently, the parsed documents do not include document as part of the strucutre id.\n        We include document id in the relevant retrieved documents for now.\n        TODO (@legendof-selda): Include document id as part of structure id in `ParsedDocumentType`.\n\n        Args:\n            document_index_map (dict[str, int]): 'document' name to index mapping.\n\n        \"\"\"\n        for chunk in self.parsed_docs:\n            assert chunk.document\n            doc_index = document_index_map[chunk.document]\n            chunk.structure = f\"{doc_index}.{chunk.structure}\"\n\n        return self.parsed_docs\n\n    def model_post_init(self, __context: Any):\n        super().model_post_init(__context)\n        # TODO (@legendof-selda): integrate the chunker class here instead.\n        # TODO (@legendof-selda): Build reverse lookups to get higher level sections easily from parsed_docs.\n        self._document_index_map, self._index_document_map = self._create_document_index()\n        self._add_doc_id(self._document_index_map)\n        # NOTE: Currently paragraph chunks seemed to work best instead of sentence.\n        self.vector_store.add_documents(\n            [\n                Document(\n                    sentence.text,\n                    metadata=dict(\n                        document=chunk.document,\n                        structure=sentence.structure,\n                        # properties=chunk.properties,\n                        key=\",\".join(sentence.key),\n                        citation_id=sentence.structure,\n                    ),\n                )\n                for chunk in self.parsed_docs\n                if isinstance(chunk, BaseTextChunk)\n                for sentence in (chunk.sentences if chunk.sentences else [chunk])\n                if isinstance(sentence, BaseTextChunk)\n            ]\n            if self.store_sentences\n            else [\n                Document(\n                    chunk.text,\n                    metadata=dict(\n                        document=chunk.document,\n                        structure=chunk.structure,\n                        # properties=chunk.properties,\n                        key=\",\".join(chunk.key),\n                        citation_id=chunk.structure,\n                    ),\n                )\n                for chunk in self.parsed_docs\n                if isinstance(chunk, BaseTextChunk)\n            ]\n        )\n\n    def _get_higher_section(self, documents: list[Document]) -&gt; list[Document]:\n        \"\"\"Utility to convert lower level structure (eg. sentences) to a higher level structure (eg. paragraphs).\n        We return only unique documents back.\n        Eg. If there are 3 sentences of the same paragraph, we only want a single paragraph document back.\n\n        Args:\n            documents (list[Document]): Relevant documents retrieved from the vector store.\n\n        Returns:\n            list[Document]: Relevant documents from the vector store, but converted to a higher level structure.\n        \"\"\"\n        # TODO (@legendof-selda): Refactor to make use of inverse lookups for faster higher strucutre retrieval.\n        return [\n            Document(\n                # TODO (@legendof-selda): this max chunk clipping is only a temp solution\n                # ideally the intelligent chunker class will take care of this based on token length.\n                chunk.text[: self.max_chunk_length],\n                metadata=dict(\n                    document=chunk.document,\n                    # properties=chunk.properties,\n                    key=\",\".join(chunk.key),\n                    citation_id=chunk.structure,\n                ),\n            )\n            # This is in paragraph level.\n            for chunk in self.parsed_docs\n            if isinstance(chunk, BaseTextChunk)\n            and any(\n                chunk.has_subsection(doc.metadata.get(\"structure\", \"\"))\n                # In sentence level.\n                for doc in documents\n            )\n        ]\n\n    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -&gt; list[Document]:\n        documents = self.vector_store_retriver._get_relevant_documents(query, run_manager=run_manager)\n        if self.store_sentences:\n            documents = self._get_higher_section(documents)\n        return documents\n\n    async def _aget_relevant_documents(\n        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n    ) -&gt; list[Document]:\n        documents = await self.vector_store_retriver._aget_relevant_documents(query, run_manager=run_manager)\n        if self.store_sentences:\n            documents = self._get_higher_section(documents)\n        return documents\n</code></pre>"},{"location":"reference/langchain/bindings/","title":"Bindings","text":""},{"location":"reference/langchain/bindings/#supermat.langchain.bindings.SupermatRetriever","title":"<code>SupermatRetriever</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Supermat Langchain Custom Retriever. This uses any Langchain VectorStore and overrides the documents retrieval methods to make it work for Supermat. NOTE: Currently this only works on Text chunks.</p> <pre><code>from supermat.langchain.bindings import SupermatRetriever\nfrom langchain_chroma import Chroma\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nretriever = SupermatRetriever(\n    parsed_docs=FileProcessor.process_file(pdf_file_path),\n    document_name=pdf_file_path.stem,\n    vector_store=Chroma(\n        embedding_function=HuggingFaceEmbeddings(\n            model_name=\"thenlper/gte-base\",\n        )\n    ),\n)\n</code></pre> <p>Args:     parsed_docs (ParsedDocumentType): The supermat parsed documents.     vector_store (VectorStore): The vector store used to store the document chunks.     vector_store_retriver_kwargs (dict[str, Any], optional): <code>VectorStore</code> kwargs used during initialization.         Defaults to <code>{}</code>.     max_chunk_length (int, optional): Max character length. NOTE: This needs to be based on tokens instead.         Defaults to 8000.     store_sentences (bool, optional): Store sentence level chunks in vector store         which will then be converted to paragraphs before sending to LLM. Defaults to False.</p> Source code in <code>supermat/langchain/bindings.py</code> <pre><code>class SupermatRetriever(BaseRetriever):\n    \"\"\"\n    Supermat Langchain Custom Retriever.\n    This uses any Langchain VectorStore and overrides the documents retrieval methods to make it work for Supermat.\n    NOTE: Currently this only works on Text chunks.\n\n\n    ``` python\n    from supermat.langchain.bindings import SupermatRetriever\n    from langchain_chroma import Chroma\n    from langchain_huggingface import HuggingFaceEmbeddings\n\n    retriever = SupermatRetriever(\n        parsed_docs=FileProcessor.process_file(pdf_file_path),\n        document_name=pdf_file_path.stem,\n        vector_store=Chroma(\n            embedding_function=HuggingFaceEmbeddings(\n                model_name=\"thenlper/gte-base\",\n            )\n        ),\n    )\n    ```\n    Args:\n        parsed_docs (ParsedDocumentType): The supermat parsed documents.\n        vector_store (VectorStore): The vector store used to store the document chunks.\n        vector_store_retriver_kwargs (dict[str, Any], optional): `VectorStore` kwargs used during initialization.\n            Defaults to `{}`.\n        max_chunk_length (int, optional): Max character length. NOTE: This needs to be based on tokens instead.\n            Defaults to 8000.\n        store_sentences (bool, optional): Store sentence level chunks in vector store\n            which will then be converted to paragraphs before sending to LLM. Defaults to False.\n    \"\"\"\n\n    parsed_docs: ParsedDocumentType = Field(exclude=True, strict=False, repr=False)\n    vector_store: VectorStore\n    vector_store_retriver_kwargs: dict[str, Any] = {}\n    max_chunk_length: int = 8000\n    store_sentences: bool = False\n\n    @cached_property\n    def vector_store_retriver(self) -&gt; VectorStoreRetriever:\n        return self.vector_store.as_retriever(**self.vector_store_retriver_kwargs)\n\n    def _create_document_index(self) -&gt; tuple[dict[str, int], dict[int, str]]:\n        documents = {\n            chunk.document\n            for chunk in self.parsed_docs\n            # NOTE: we assume that all chunks have document\n            if chunk.document is not None\n        }\n        # NOTE: we want the document id to start with 1, since 0 means all in structure id.\n        document_index_map = {document: doc_id for doc_id, document in enumerate(documents, 1)}\n        index_document_map = dict(zip(document_index_map.values(), document_index_map.keys()))\n        return document_index_map, index_document_map\n\n    def _add_doc_id(self, document_index_map: dict[str, int]):\n        \"\"\"\n        Mutates current `parsed_docs` to include document id in the chunk structure id.\n        This is a temporary solution.\n        Currently, the parsed documents do not include document as part of the strucutre id.\n        We include document id in the relevant retrieved documents for now.\n        TODO (@legendof-selda): Include document id as part of structure id in `ParsedDocumentType`.\n\n        Args:\n            document_index_map (dict[str, int]): 'document' name to index mapping.\n\n        \"\"\"\n        for chunk in self.parsed_docs:\n            assert chunk.document\n            doc_index = document_index_map[chunk.document]\n            chunk.structure = f\"{doc_index}.{chunk.structure}\"\n\n        return self.parsed_docs\n\n    def model_post_init(self, __context: Any):\n        super().model_post_init(__context)\n        # TODO (@legendof-selda): integrate the chunker class here instead.\n        # TODO (@legendof-selda): Build reverse lookups to get higher level sections easily from parsed_docs.\n        self._document_index_map, self._index_document_map = self._create_document_index()\n        self._add_doc_id(self._document_index_map)\n        # NOTE: Currently paragraph chunks seemed to work best instead of sentence.\n        self.vector_store.add_documents(\n            [\n                Document(\n                    sentence.text,\n                    metadata=dict(\n                        document=chunk.document,\n                        structure=sentence.structure,\n                        # properties=chunk.properties,\n                        key=\",\".join(sentence.key),\n                        citation_id=sentence.structure,\n                    ),\n                )\n                for chunk in self.parsed_docs\n                if isinstance(chunk, BaseTextChunk)\n                for sentence in (chunk.sentences if chunk.sentences else [chunk])\n                if isinstance(sentence, BaseTextChunk)\n            ]\n            if self.store_sentences\n            else [\n                Document(\n                    chunk.text,\n                    metadata=dict(\n                        document=chunk.document,\n                        structure=chunk.structure,\n                        # properties=chunk.properties,\n                        key=\",\".join(chunk.key),\n                        citation_id=chunk.structure,\n                    ),\n                )\n                for chunk in self.parsed_docs\n                if isinstance(chunk, BaseTextChunk)\n            ]\n        )\n\n    def _get_higher_section(self, documents: list[Document]) -&gt; list[Document]:\n        \"\"\"Utility to convert lower level structure (eg. sentences) to a higher level structure (eg. paragraphs).\n        We return only unique documents back.\n        Eg. If there are 3 sentences of the same paragraph, we only want a single paragraph document back.\n\n        Args:\n            documents (list[Document]): Relevant documents retrieved from the vector store.\n\n        Returns:\n            list[Document]: Relevant documents from the vector store, but converted to a higher level structure.\n        \"\"\"\n        # TODO (@legendof-selda): Refactor to make use of inverse lookups for faster higher strucutre retrieval.\n        return [\n            Document(\n                # TODO (@legendof-selda): this max chunk clipping is only a temp solution\n                # ideally the intelligent chunker class will take care of this based on token length.\n                chunk.text[: self.max_chunk_length],\n                metadata=dict(\n                    document=chunk.document,\n                    # properties=chunk.properties,\n                    key=\",\".join(chunk.key),\n                    citation_id=chunk.structure,\n                ),\n            )\n            # This is in paragraph level.\n            for chunk in self.parsed_docs\n            if isinstance(chunk, BaseTextChunk)\n            and any(\n                chunk.has_subsection(doc.metadata.get(\"structure\", \"\"))\n                # In sentence level.\n                for doc in documents\n            )\n        ]\n\n    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -&gt; list[Document]:\n        documents = self.vector_store_retriver._get_relevant_documents(query, run_manager=run_manager)\n        if self.store_sentences:\n            documents = self._get_higher_section(documents)\n        return documents\n\n    async def _aget_relevant_documents(\n        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun\n    ) -&gt; list[Document]:\n        documents = await self.vector_store_retriver._aget_relevant_documents(query, run_manager=run_manager)\n        if self.store_sentences:\n            documents = self._get_higher_section(documents)\n        return documents\n</code></pre>"},{"location":"reference/langchain/bindings/#supermat.langchain.bindings.get_default_chain","title":"<code>get_default_chain(retriever, llm_model, substitute_references=False, return_context=False)</code>","text":"<p>Default chain that implements citation where LLM returns the referenced id as well instead of directly returning the values verbatim. This saves output tokens being generated and the actual content is returned during post processing.</p> <p>Parameters:</p> Name Type Description Default <code>retriever</code> <code>SupermatRetriever</code> <p>SupermatRetriever that retrieves the relevant document chunks for LLM context.</p> required <code>llm_model</code> <code>BaseChatModel | BaseLLM</code> <p>The LLM model used for inference</p> required <code>substitute_references</code> <code>bool</code> <p>Whether to replace the citations direction, or as a separate section. Defaults to False.</p> <code>False</code> <code>return_context</code> <code>bool</code> <p>Return retrived documents for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RunnableSerializable</code> <code>RunnableSerializable</code> <p>Langchain chain to run prompt query.</p> Source code in <code>supermat/langchain/bindings.py</code> <pre><code>def get_default_chain(\n    retriever: SupermatRetriever,\n    llm_model: BaseLanguageModel,\n    substitute_references: bool = False,\n    return_context: bool = False,\n) -&gt; RunnableSerializable:\n    \"\"\"Default chain that implements citation where LLM returns the referenced id as well\n    instead of directly returning the values verbatim. This saves output tokens being generated and the actual content\n    is returned during post processing.\n\n    Args:\n        retriever (SupermatRetriever): SupermatRetriever that retrieves the relevant document chunks for LLM context.\n        llm_model (BaseChatModel | BaseLLM): The LLM model used for inference\n        substitute_references (bool, optional): Whether to replace the citations direction, or as a separate section.\n            Defaults to False.\n        return_context (bool, optional): Return retrived documents for debugging. Defaults to False.\n\n    Returns:\n        RunnableSerializable: Langchain chain to run prompt query.\n    \"\"\"\n    prompt = get_default_prompt()\n    chain = RunnableParallel({\"context\": retriever | pre_format_docs, \"question\": RunnablePassthrough()}) | {\n        \"llm_output\": prompt.partial(context=itemgetter(\"context\") | RunnableLambda(format_docs))\n        | llm_model\n        | StrOutputParser(),\n        \"context\": itemgetter(\"context\"),\n    }\n    _post_process = RunnableLambda(partial(post_process, substitute=substitute_references))\n    if return_context:\n        chain |= {\n            \"llm_output\": itemgetter(\"llm_output\"),\n            \"answer\": _post_process,\n            \"context\": itemgetter(\"context\"),\n            \"formatted_context\": (itemgetter(\"context\") | RunnableLambda(format_docs)),\n        }\n    else:\n        chain |= _post_process\n\n    return chain\n</code></pre>"},{"location":"reference/langchain/bindings/#supermat.langchain.bindings.parse_cite_blocks","title":"<code>parse_cite_blocks(text)</code>","text":"<p>Parses the <code>&lt;cite ref='citation_id', start=0, end=None /&gt;</code> cite block in a text. NOTE: Could not get the LLM to return start and end via prompt templating.    This is a demo to show that citations are possible, and thus reduces output tokens from llm. With citations, we can avoid llm's returning tokens which are already available in context.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text containing the cite block</p> required <p>Returns:</p> Type Description <code>tuple[ParsedCite]</code> <p>tuple[ParsedCite]: Parsed citations found in text.</p> Source code in <code>supermat/langchain/bindings.py</code> <pre><code>def parse_cite_blocks(text: str) -&gt; tuple[ParsedCite]:\n    \"\"\"Parses the `&lt;cite ref='citation_id', start=0, end=None /&gt;` cite block in a text.\n    NOTE: Could not get the LLM to return start and end via prompt templating.\\\n    This is a demo to show that citations are possible, and thus reduces output tokens from llm.\n    With citations, we can avoid llm's returning tokens which are already available in context.\n\n    Args:\n        text (str): Text containing the cite block\n\n    Returns:\n        tuple[ParsedCite]: Parsed citations found in text.\n    \"\"\"\n    pattern = r\"(&lt;cite ref='([^']*)'(,?\\s*start=(\\d+))?(,?\\s*end=(\\d+))?\\s*/&gt;)\"\n    matches = re.findall(pattern, text)\n    parsed_blocks = set()\n    for match in matches:\n        block = {\"cite_block\": match[0], \"ref\": match[1]}\n        if match[3]:\n            block[\"start\"] = int(match[3])\n        if match[5]:\n            block[\"end\"] = int(match[5])\n        parsed_blocks.add(ParsedCite(**block))\n\n    return tuple(parsed_blocks)\n</code></pre>"},{"location":"reference/langchain/bindings/#supermat.langchain.bindings.post_process","title":"<code>post_process(chain_output, substitute=False)</code>","text":"<p>LLM chain link to replace references with content using regex matching.</p> <p>Parameters:</p> Name Type Description Default <code>chain_output</code> <code>ChainOutput</code> <p>Output of previous Lanchain link.</p> required <code>substitute</code> <code>bool</code> <p>Substitute the reference matched directly. Defaults to False. If, substitute is False, in a new paragraph, the referenced chunk is dumped directly.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns model output with reference ids parsed to actual content.</p> Source code in <code>supermat/langchain/bindings.py</code> <pre><code>def post_process(chain_output: ChainOutput, substitute: bool = False) -&gt; str:\n    \"\"\"LLM chain link to replace references with content using regex matching.\n\n    Args:\n        chain_output (ChainOutput): Output of previous Lanchain link.\n        substitute (bool, optional): Substitute the reference matched directly. Defaults to False.\n            If, substitute is False, in a new paragraph, the referenced chunk is dumped directly.\n\n    Returns:\n        str: Returns model output with reference ids parsed to actual content.\n    \"\"\"\n    output = chain_output[\"llm_output\"]\n    doc_mapping = {doc.metadata[\"citation_id\"]: doc for doc in chain_output[\"context\"]}\n\n    references_used = parse_cite_blocks(output)\n    total_references = len(references_used)\n\n    def get_reference_quote(parsed_cite: ParsedCite) -&gt; str | None:\n        if parsed_cite.ref not in doc_mapping:\n            return None\n        reference = doc_mapping[parsed_cite.ref]\n        return reference.page_content[parsed_cite.start : parsed_cite.end]\n\n    if substitute:\n        processed_output = output\n        for parsed_cite in references_used:\n            reference = (\n                ref if (ref := get_reference_quote(parsed_cite)) else f\"&lt;invalid_ref# {parsed_cite.cite_block}/&gt;\"\n            )\n            processed_output = processed_output.replace(parsed_cite.cite_block, reference, 1)\n    else:\n\n        def get_reference(parsed_cite: ParsedCite) -&gt; str:\n            return (\n                ref + \"\\n\" + pformat(doc_mapping[parsed_cite.ref].model_dump())\n                if (ref := get_reference_quote(parsed_cite))\n                else f\"[Reference not found: {parsed_cite.cite_block}]\"\n            )\n\n        references = [\n            f\"{i:0{total_references}}. {parsed_cite.cite_block}:\\n\\t{get_reference(parsed_cite)}\\n\"\n            for i, parsed_cite in enumerate(references_used, 1)\n        ]\n        references_section = \"\\n\\nReferences:\\n\" + (\"\\n\".join(references))\n\n        processed_output = output + references_section\n\n    return processed_output\n</code></pre>"},{"location":"reference/langchain/metrics/","title":"Metrics","text":""},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates Accuracy metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Accuracy(RunEvaluator):\n    \"\"\"Evaluates Accuracy metric\"\"\"\n\n    def __init__(self, llm: BaseChatModel) -&gt; None:\n        self.llm = llm\n        self.evaluator_accuracy = load_evaluator(\n            \"labeled_score_string\",  # type: ignore\n            criteria={\n                \"accuracy\": \"\"\"\n                Score 1: The answer is completely unrelated to the reference.\n                Score 3: The answer has minor relevance but does not align with the reference.\n                Score 5: The answer has moderate relevance but contains inaccuracies.\n                Score 7: The answer aligns with the reference but has minor errors or omissions.\n                Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\",\n            },\n            llm=self.llm,\n            normalize_by=10,\n        )\n\n    def evaluate_run(self, run: Run, example: Example) -&gt; EvaluationResult | EvaluationResults:\n        res = self.evaluator_accuracy.evaluate_strings(\n            prediction=next(iter(run.outputs.values())),\n            input=run.inputs[\"Question\"],\n            # We are treating the documents as the reference context in this case.\n            reference=example.outputs[\"Answer\"],\n        )\n        return EvaluationResult(key=\"labeled_criteria:accuracy\", **res)\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.CosineSimilarity","title":"<code>CosineSimilarity</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates cosine similarity metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class CosineSimilarity(RunEvaluator):\n    \"\"\"Evaluates cosine similarity metric\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=\"thenlper/gte-base\",\n        )\n\n    def evaluate_run(self, run: Run, example: Example | None = None):\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n\n        response_embedding = np.array(self.embedding_model.embed_query(response))\n        reference_embedding = np.array(self.embedding_model.embed_query(reference))\n\n        dot_product = np.dot(response_embedding, reference_embedding)\n        cosine_similarity = dot_product / (np.linalg.norm(response_embedding)) * (np.linalg.norm(reference_embedding))\n        return EvaluationResult(\n            **{\n                \"key\": \"cosine_similarity\",\n                \"score\": cosine_similarity,\n            }\n        )\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.FaithfullnessMetrics","title":"<code>FaithfullnessMetrics</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates Faithfullness metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class FaithfullnessMetrics(RunEvaluator):\n    \"\"\"Evaluates Faithfullness metric\"\"\"\n\n    def __init__(self, llm: BaseChatModel) -&gt; None:\n        self.llm = llm\n        self.evaluator_faithfullness = load_evaluator(\n            \"labeled_score_string\",\n            criteria={\n                \"faithful\": \"How faithful is the submission to the reference context?\",\n            },\n            llm=self.llm,\n            normalize_by=10,\n        )\n\n    def evaluate_run(self, run: Run, example: Example) -&gt; dict:\n        res = self.evaluator_faithfullness.evaluate_strings(\n            prediction=next(iter(run.outputs.values())),\n            input=run.inputs[\"Question\"],\n            reference=example.inputs[\"documents\"],\n        )\n        return EvaluationResult(key=\"labeled_criteria:faithful\", **res)\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge1","title":"<code>Rouge1</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates ROGUE1 F1 metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge1(RunEvaluator):\n    \"\"\"Evaluates ROGUE1 F1 metric\"\"\"\n\n    def __init__(self) -&gt; None:\n        # \"ROUGE-Lsum splits the text into sentences based on newlines\n        # and computes the LCS for each pair of sentences and take the average score for all sentences\n        self.scorer = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\n        self.score_func = cache(self.scorer.score)\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge1_f1_score\",\n                \"score\": rouge_score[\"rouge1\"].fmeasure,\n                \"comment\": f\"precision:{rouge_score['rouge1'].precision}, recall:{rouge_score['rouge1'].recall}\",\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge1Precision","title":"<code>Rouge1Precision</code>","text":"<p>               Bases: <code>Rouge1</code></p> <p>Evaluates ROGUE1 precision metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge1Precision(Rouge1):\n    \"\"\"Evaluates ROGUE1 precision metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge1_precision\",\n                \"score\": rouge_score[\"rouge1\"].precision,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge1Recall","title":"<code>Rouge1Recall</code>","text":"<p>               Bases: <code>Rouge1</code></p> <p>Evaluates ROGUE1 recall metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge1Recall(Rouge1):\n    \"\"\"Evaluates ROGUE1 recall metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge1_recall\",\n                \"score\": rouge_score[\"rouge1\"].recall,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge2","title":"<code>Rouge2</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates ROGUE2 F1 metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge2(RunEvaluator):\n    \"\"\"Evaluates ROGUE2 F1 metric\"\"\"\n\n    def __init__(self) -&gt; None:\n        # \"ROUGE-Lsum splits the text into sentences based on newlines\n        # and computes the LCS for each pair of sentences and take the average score for all sentences\n        self.scorer = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n        self.score_func = cache(self.scorer.score)\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge2_f1_score\",\n                \"score\": rouge_score[\"rouge2\"].fmeasure,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge2Precision","title":"<code>Rouge2Precision</code>","text":"<p>               Bases: <code>Rouge2</code></p> <p>Evaluates ROGUE2 precision metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge2Precision(Rouge2):\n    \"\"\"Evaluates ROGUE2 precision metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge2_precision\",\n                \"score\": rouge_score[\"rouge2\"].precision,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.Rouge2Recall","title":"<code>Rouge2Recall</code>","text":"<p>               Bases: <code>Rouge2</code></p> <p>Evaluates ROGUE2 recall metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class Rouge2Recall(Rouge2):\n    \"\"\"Evaluates ROGUE2 recall metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rouge2_recall\",\n                \"score\": rouge_score[\"rouge2\"].recall,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.RougeLsum","title":"<code>RougeLsum</code>","text":"<p>               Bases: <code>RunEvaluator</code></p> <p>Evaluates ROGUE-L F1 metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class RougeLsum(RunEvaluator):\n    \"\"\"Evaluates ROGUE-L F1 metric\"\"\"\n\n    def __init__(self) -&gt; None:\n        # \"ROUGE-Lsum splits the text into sentences based on newlines\n        # and computes the LCS for each pair of sentences and take the average score for all sentences\n        self.scorer = rouge_scorer.RougeScorer([\"rougeLsum\"], use_stemmer=True)\n        self.score_func = cache(self.scorer.score)\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rougeLsum_f1_score\",\n                \"score\": rouge_score[\"rougeLsum\"].fmeasure,\n                \"comment\": f\"precision:{rouge_score['rougeLsum'].precision}, recall:{rouge_score['rougeLsum'].recall}\",\n            },\n        )\n\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.RougeLsumPrecision","title":"<code>RougeLsumPrecision</code>","text":"<p>               Bases: <code>RougeLsum</code></p> <p>Evaluates ROGUE-L sum precision metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class RougeLsumPrecision(RougeLsum):\n    \"\"\"Evaluates ROGUE-L sum precision metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rougeLsum_precision\",\n                \"score\": rouge_score[\"rougeLsum\"].precision,\n            },\n        )\n        return result\n</code></pre>"},{"location":"reference/langchain/metrics/#supermat.langchain.metrics.RougeLsumRecall","title":"<code>RougeLsumRecall</code>","text":"<p>               Bases: <code>RougeLsum</code></p> <p>Evaluates ROGUE-L sum Recall metric</p> Source code in <code>supermat/langchain/metrics.py</code> <pre><code>class RougeLsumRecall(RougeLsum):\n    \"\"\"Evaluates ROGUE-L sum Recall metric\"\"\"\n\n    def evaluate_run(self, run: Run, example: Example | None = None) -&gt; EvaluationResult | EvaluationResults:\n        response = run.outputs[\"output\"]\n        reference = example.outputs[\"Answer\"]\n        rouge_score = self.scorer.score(target=reference, prediction=response)\n\n        result = EvaluationResult(\n            **{\n                \"key\": \"rougeLsum_recall\",\n                \"score\": rouge_score[\"rougeLsum\"].recall,\n            },\n        )\n        return result\n</code></pre>"}]}