{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def override_sys_breakpoint(frame=None):\n",
    "    from IPython.core.debugger import set_trace\n",
    "\n",
    "    set_trace(frame=frame)\n",
    "\n",
    "\n",
    "sys.breakpointhook = override_sys_breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUAD_PATH = Path(\"../data/CUAD_v1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "CUAD_QNA_PATH = Path(os.getenv(\"CUAD_QNA_PATH\"))\n",
    "CUAD_QNA_SUBSET_PATH = Path(os.getenv(\"CUAD_QNA_SUBSET_PATH\"))\n",
    "\n",
    "# os.environ[\"LANGCHAIN_TRACING\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files=list(CUAD_QNA_SUBSET_PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supermat.core.parser import FileProcessor\n",
    "from tqdm.auto import tqdm\n",
    "from typing import TYPE_CHECKING, cast\n",
    "from itertools import chain\n",
    "from joblib import Parallel, delayed\n",
    "parsed_files = Parallel(n_jobs=-1)(\n",
    "    delayed(FileProcessor.parse_file)(path) for path in pdf_files\n",
    ")\n",
    "if TYPE_CHECKING:\n",
    "    from supermat.core.models.parsed_document import ParsedDocumentType\n",
    "    parsed_files = cast(list[ParsedDocumentType], parsed_files)\n",
    "\n",
    "documents = list(chain.from_iterable(parsed_docs for parsed_docs in parsed_files))\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from supermat.core.models.parsed_document import ParsedDocumentType\n",
    "    documents = cast(ParsedDocumentType, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supermat.langchain.bindings import SupermatRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "retriever = SupermatRetriever(\n",
    "    parsed_docs=documents,\n",
    "    vector_store=Chroma(\n",
    "        embedding_function=HuggingFaceEmbeddings(\n",
    "            model_name=\"thenlper/gte-base\",\n",
    "        ),\n",
    "        persist_directory=\"./chromadb\",\n",
    "        collection_name=\"CUAD_TEST\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.smith.evaluation.runner_utils import TestResult\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.runnables.base import Runnable\n",
    "\n",
    "from supermat.langchain.metrics import (\n",
    "    Accuracy,\n",
    "    CosineSimilarity,\n",
    "    FaithfullnessMetrics,\n",
    "    Rouge1,\n",
    "    Rouge1Precision,\n",
    "    Rouge1Recall,\n",
    "    Rouge2,\n",
    "    Rouge2Precision,\n",
    "    Rouge2Recall,\n",
    "    RougeLsum,\n",
    "    RougeLsumPrecision,\n",
    "    RougeLsumRecall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Question: {Question}\n",
    "context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_TEMPLATE = \"\"\"\n",
    "# Use the below information to answer the subsequent question.\n",
    "# Please cite the structure number of the most relevant document(s) found inside the metadata. Other metadata info need not be returned.\n",
    "# Provide factual, verifiable information and include references to credible sources where possible. \n",
    "# Avoid speculative or unverified content.\n",
    "# If the answer cannot be found, write \"I don't know.\"\n",
    "# Information:\n",
    "# \\\"\\\"\\\"\n",
    "# {context}\n",
    "# \\\"\\\"\\\"\n",
    "# Question: {Question}? Please cite the each structure numbers you take data from. Please provide factual and verifiable information, include references to credible sources where possible. \n",
    "# Avoid speculative or unverified content.\n",
    "# Return a JSON object as output.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_TEMPLATE=\"\"\"\n",
    "# {\n",
    "#   \"task\": \"Use the context info to answer the given question. Please cite the structure number of the most relevant document(s) found inside the metadata. Other metadata info need not be returned. Provide factual, verifiable information and include references to credible sources where possible. Avoid speculative or unverified content. If the answer cannot be found, write 'I don't know.'\",\n",
    "#   \"question\": {Question},\n",
    "#   \"context\": {context}\n",
    "# }\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# llm_model = AzureChatOpenAI(azure_deployment='gpt-35-turbo',api_version=\"2024-05-01-preview\", temperature=0, model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\n",
    "llm_model = AzureChatOpenAI(azure_deployment='gpt-35-turbo',api_version=\"2024-05-01-preview\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.cache import BaseCache\n",
    "from langchain_core.callbacks.base import Callbacks\n",
    "\n",
    "RunEvalConfig.LabeledScoreString.model_rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{Question}\"),\n",
    "    ]\n",
    ")\n",
    "import json\n",
    "\n",
    "def format_docs(docs: list[Document]) -> str:\n",
    "    response = [f\"{{'text':{doc.page_content}, 'metadata': {json.dumps(doc.metadata)}}}\" for doc in docs]\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "qa_chain2 = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"Question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "qa_chain2 = (\n",
    "    RunnableLambda(lambda x: x[\"Question\"])\n",
    "    | RunnableParallel({\"context\": retriever | format_docs, \"Question\": RunnablePassthrough()})\n",
    ")\n",
    "qa_chain2 = (\n",
    "    RunnableLambda(lambda x: x[\"Question\"])\n",
    "    | RunnableParallel({\"context\": retriever | format_docs, \"Question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from langchain_benchmarks.extraction.evaluators import get_eval_config\n",
    "\n",
    "rag_evaluation = get_eval_config(llm_model)\n",
    "eval_config = RunEvalConfig.model_validate(\n",
    "    rag_evaluation.model_dump()\n",
    "    | RunEvalConfig(\n",
    "        custom_evaluators=[\n",
    "            FaithfullnessMetrics(llm_model),\n",
    "            Accuracy(llm_model),\n",
    "            CosineSimilarity(),\n",
    "            Rouge1(),\n",
    "            Rouge1Precision(),\n",
    "            Rouge1Recall(),\n",
    "            Rouge2(),\n",
    "            Rouge2Precision(),\n",
    "            Rouge2Recall(),\n",
    "            RougeLsum(),\n",
    "            RougeLsumPrecision(),\n",
    "            RougeLsumRecall(),\n",
    "        ],\n",
    "        input_key=\"Question\",\n",
    "    ).model_dump()\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    RunnableLambda(lambda x: x[\"Question\"])\n",
    "    | RunnableParallel({\"context\": retriever, \"Question\": RunnablePassthrough()})\n",
    "    # | RunnableLambda(lambda x: {\n",
    "    #     \"context\": \" \".join(f\"{doc.page_content}\\nSection: {doc.metadata[\"structure\"]}\\nKeys: {doc.metadata[\"key\"]}\" for doc in x[\"context\"]),\n",
    "    #     \"Question\": x[\"Question\"]\n",
    "    # })\n",
    "    | ChatPromptTemplate.from_template(DEFAULT_TEMPLATE)\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "from langchain_benchmarks.utils import run_without_langsmith\n",
    "\n",
    "from importlib import reload\n",
    "from langchain_benchmarks import utils\n",
    "reload(utils)\n",
    "\n",
    "test_run = utils.run_without_langsmith(\n",
    "    path_or_token_id=CUAD_QNA_PATH.as_posix(),\n",
    "    llm_or_chain_factory=qa_chain2,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    concurrency_level=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_TEMPLATE=\"\"\"\n",
    "# {\n",
    "#   \"task\": \"Use the context info to answer the given question. Please cite the structure number of the most relevant document(s) found inside the metadata. Other metadata info need not be returned. Provide factual, verifiable information and include references to credible sources where possible. Avoid speculative or unverified content. If the answer cannot be found, write 'I don't know.'\",\n",
    "#   \"question\": {Question},\n",
    "#   \"context\": {context}\n",
    "# }\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestFormat(BaseModel):\n",
    "    task: str = Field(\"Use the context info to answer the given question. Please cite the structure number of the most relevant document(s) found inside the metadata. Other metadata info need not be returned. Provide factual, verifiable information and include references to credible sources where possible. Avoid speculative or unverified content. If the answer cannot be found, write 'I don't know.'\", description=\"The answer to the user's question.\")\n",
    "    Document: str = Field(description=\"The user question.\")\n",
    "    context: str = Field(description=\"The list of documents used as context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ResponseFormat(BaseModel):\n",
    "#     answer: str = Field(description=\"The answer to the user's question.\")\n",
    "#     Document: str = Field(description=\"The document name the info was taken from.\")\n",
    "#     structure: str = Field(description=\"The structure number(s) the information was taken from.\")\n",
    "\n",
    "# qa_chain2 = (\n",
    "#     RunnableLambda(lambda x: x[\"Question\"])\n",
    "#     | RunnableParallel({\"context\": retriever, \"Question\": RunnablePassthrough()})\n",
    "#     # | RunnableLambda(lambda x: {\n",
    "#     #     \"context\": \" \".join(f\"{doc.page_content}\\nSection: {doc.metadata[\"structure\"]}\\nKeys: {doc.metadata[\"key\"]}\" for doc in x[\"context\"]),\n",
    "#     #     \"Question\": x[\"Question\"]\n",
    "#     # })\n",
    "#     | ChatPromptTemplate.from_template(DEFAULT_TEMPLATE)\n",
    "#     # | llm_model\n",
    "#     # | StrOutputParser()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "with CUAD_QNA_PATH.with_name(\"full_cuad.json\").open(\"rb\") as fp:\n",
    "    data = orjson.loads(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_chain = (\n",
    "    RunnableLambda(lambda x: x[\"Question\"])\n",
    "    | RunnableParallel({\"context\": retriever, \"Question\": RunnablePassthrough()})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_chain.invoke(data[0][\"inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_run is not None\n",
    "\n",
    "run_agg = test_run.get_aggregate_feedback()\n",
    "run_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baseline(baseline_pkl_path: Path, run_agg: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    baseline_agg_results = pd.read_pickle(baseline_pkl_path)\n",
    "    baseline_agg_results = baseline_agg_results = baseline_agg_results.droplevel([\"llm_model\", \"vectorstore_name\", \"dataset\"], axis=1)\n",
    "    diff_to_baseline = (run_agg.select_dtypes(\"float64\") - baseline_agg_results.select_dtypes(\"float64\"))/baseline_agg_results.select_dtypes(\"float64\")\n",
    "    return baseline_agg_results, diff_to_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks_file_name = \"baseline_benchmarks\"\n",
    "benchmarks_file_name = \"baseline_SemanticChunkerPercentile_benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_agg_results, diff_to_baseline = compare_baseline(fr\"C:\\repos\\llm_rag\\notebooks\\{benchmarks_file_name}.pkl\", run_agg)\n",
    "diff_to_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"supermat_benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert test_run is not None\n",
    "\n",
    "with pd.ExcelWriter(f\"{file_name}.xlsx\") as writer:\n",
    "    test_run.to_dataframe().to_excel(writer, sheet_name=\"LLM Results\", index=True)\n",
    "    run_agg.to_excel(writer, sheet_name=\"Agg Results\", index=True)\n",
    "    diff_to_baseline.to_excel(writer, sheet_name=\"Baseline Diff Agg Results\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agg.to_pickle(f\"{file_name}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supermat12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
